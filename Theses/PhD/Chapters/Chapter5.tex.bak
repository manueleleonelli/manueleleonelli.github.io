% Chapter Template

\chapter{The Algebra of Integrated Partial Belief Systems} % Main chapter title

\label{chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{The Algebra of Integrated Partial Belief Systems}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

The examples of Section \ref{sec:exalgo} showed that the \gls{IDSS}'s expected utilities are often polynomial functions whose indeterminates are individually delivered by the panels. By requesting only this information, the implementation of an \gls{IDSS} can become of orders of magnitude more manageable. This is because panels just need to communicate a few summaries from their sample: a trivial and fast task to perform. 

Therefore the full inferential conditions guaranteeing a sound and distributed analysis of Chapter \ref{chapter3} are actually too strict for the sole purpose of decision support. These can then be relaxed, whilst retaining the coherence of the \gls{IDSS}. By taking an algebraic approach we develop in this chapter a methodology to ensure coherence in these types of partially defined systems, meaning systems where panels only deliver certain selected summaries. The algebraic structure of the \gls{IDSS} expected utilities allows us to impose new independence conditions tailored to the needs of \glspl{IDSS}, consisting of the requirement that certain summaries simply satisfy certain polynomial relationships. 

Although this is not often recognized in the literature, algebraic approaches are intimately linked to symbolic ones, where probabilities are treated as indeterminates in a computer algebra system, thus not requiring an exact numeric specification. Symbolic inference is often used in sensitivity analysis to identify the probabilities \glspl{DM} need to be particularly precise in their elicitation, as these might drastically change the ranking of the available policies \citep{French2003}. 

In this chapter we study how algebraic and symbolic methods can be used within the framework of \glspl{IDSS}. The structure of the chapter is as follows. In Section \ref{sec:history} we  review the main developments in symbolic and algebraic inferential methods. In Section \ref{sec:description} we  define algebraically the  expected utility of an  \glspl{IDSS}. Section \ref{sec:momind} introduces polynomial conditions the panels' summaries need to entertain and we then show in Section \ref{sec:adequacy1} how these are able to guarantee adequacy and distributivity. Section \ref{sec:parexamples} presents a variety of examples of these methods, including \glspl{GBN} and Bayes linear graphical models. In Section \ref{sec:short} we briefly review recent symbolic methods for two particular integrating structures: \glspl{ID} and staged trees. We conclude with a discussion.
 
\section{A Brief Overview of Symbolic and Algebraic Methods in Statistics}
\label{sec:history}
As we briefly mentioned in the introduction, symbolic and algebraic methods for statistical inference have been developed independently by two different community of researchers. The first one has been the focus of mostly computer scientists, whilst the second of algebraists and statisticians. However in both cases, the probabilities associated to a statistical model are described as polynomials, whose structure is then exploited to study the inferential properties of such model in a variety of applications. 

Symbolic methods focused mostly on graphical models and specifically on \glspl{BN}, both discrete \citep{Castillo1995,Castillo1997a} and Gaussian \citep{Castillo1997}. Probabilities and moments associated to vertices are thought of as an indeterminate of a computer algebra system (such as Maple or Matlab), and the model's probabilities are polynomials of such indeterminates.  \citet{Castillo1996a} and \citet{Castillo2003} developed and implemented algorithms for the computation of \glspl{BN}' probability polynomials. In these works bounds that the probabilities of the vertices of the network have to respect were also identified, which can then be used in sensitivity analyses. The fundamental idea behind this approach lies in the fact that once the qualitative structure of the model is constructed, numerical specifications of the involved probabilities do not need to be delivered a priori for the algorithms to run. Furthermore the above mentioned bounds inform the \gls{DM} about probability values that cannot be delivered, thus simplifying the elicitation process. 

The symbolic inferential algorithms of \citet{Castillo1997b} had several drawbacks. Most importantly these would compute every possible monomial deriving from the symbolic definition of the model and then drop those associated to non compatible instantiations. This implies the computation of the Cartesian product  of the sets of probabilities associated to the vertices of the \gls{BN}, which can become infeasible as the size of the network grows. In \citet{Darwiche2003} a new symbolic inferential algorithm speeds up the computation of the probability polynomials, by converting the underlying \gls{BN} into an \gls{AC}. The circuit can then be evaluated through fast routines to output the required polynomials. Importantly, \citet{Darwiche2003} showed that a large number of probabilistic queries can be answered by computing certain derivatives of the probability polynomials, which are then easier to compute since the associated \gls{AC} has a smaller dimension. Such methods have been also recently extended to \glspl{DBN} \citep{Brandherm2004}.

Beside the possibility of non eliciting probabilities before any inference takes place, another great advantage of symbolic approaches is that these lead straightforwardly to various sensitivity analyses. This is because users can simply plug in into the probability indeterminates different numbers and observe how these change the inferential results on a set of goal variables. Much more sophisticated sensitivity techniques have now been implemented \citep{Chan2001,Chan2004}. 

Although the computing capability of modern computers is constantly growing, it has been possible to apply symbolic approaches only in small-medium sized applications. This is because each probability specification is treated as an unknown variable in a computer system, thus requiring much more memory space than a single number. Furthermore, as shown by \citet{Cooper1990}, the number of monomials in \gls{BN} models grows exponentially with the number of vertices, making the computation of such polynomials often prohibitive. However we note here that the technology of \glspl{IDSS} allows us to apply symbolic methods to much larger models, since one can define symbolically only the outputs of each component \gls{DSS}, which is the only relevant information an \gls{IDSS} needs to process. At the same time, this symbolic definition definition leads straightforwardly to the use of sensitivity analyses for the outputs of \glspl{IDSS}, which can be extremely useful to build consensus among the panel members. 

The second stream of research using polynomial techniques in statistics is usually referred to as \textit{algebraic statistics}. This uses techniques from, to cite a few, algebraic geometry and computational commutative algebra to gain insights into the structure of certain statistical models \citep{Riccomagno2009a}. The first developments of algebraic statistics were on  the use of both Gr\"{o}bner bases in Markov chain sampling algorithms \citep{Diaconis1998} and commutative algebra techniques in the design and analysis of experiments \citep{Pistone1996}. 

Importantly algebraic statistics also started to focus on statistical modelling. Certain statistical models have been identified with certain algebraic varieties \citep{Pistone2002}. Big part of the research in this area then focused on certain graphical models, as for example \glspl{BN} \citep{Garcia2005, Sullivant2008}, \glspl{MN} \citep{Geiger2006} and trees \citep{Settimi2000}. The number of applications of algebraic methods in statistics is now countless \citep[see for example the monographs,][]{Gibilisco2010, Pistone2002, Drton2009b, Pachter2005}.

To our knowledge, the methods we develop in the following sections are the first application of symbolic and algebraic techniques in Bayesian decision making.  

\section{An Algebraic Description of Integrating Systems}
\label{sec:description}
The computation of the expected utilities in the examples of Section \ref{sec:exalgo} showed the difficulty of identifying the beliefs that need to be delivered by the panels to the \gls{IDSS}. The situation becomes even more complicated when the utility consensus include the multilinear or the multiplicative utility factorizations introduced in Definitions \ref{def:multilinear} and \ref{def:multiplicative} respectively (unless all panels can simply define independent marginal models for the variables under their jurisdiction). 

Approaching the problem from an algebraic viewpoint allows us to identify the necessary panels' summaries and the required assumptions for adequacy. In order to do this we first need to define the expected utility polynomials.

\begin{definition}
The conditional expected utility $\bar{u}(\bm{d}\;|\;\bm{\theta})$ of an \gls{IDSS} is called \textbf{algebraic} if, for each $\bm{d}\in \bm{\mathcal{D}}$ and $\bm{\theta} \in \bm{\Theta} $, $\bar{u}(\bm{d}\;|\;\bm{\theta})$ is a square free polynomial 
\[
f_{\bm{d}}\left( \bm{\lambda}_{1}(\bm{\theta}_{1},\bm{d}),\cdots,\bm{\lambda}_m(\bm{\theta }_{m},\bm{d})\right) 
\]
 in functions $\bm{\lambda }_{_{i}}(\bm{\theta }_{i},\bm{d})$ of the parameters $\bm{\theta }_{_{i}}$ of panel $G_{i}$, $i\in[m]$.
\end{definition}

We now define the algebraic conditional expected utilities $f_{\bm{d}}\left( \cdot\right) $ above explicitly. Let $\bm{\lambda}_i(\bm{\theta}_i,\bm{d})=(\lambda _{ji}(\bm{\theta }_{i},\bm{d}),\dots ,\lambda_{s_{i}i}(\bm{\theta }_{i},\bm{d}))_{j\in[s_i]}$ and $\bm{b}\in B=\bigtimes_{i\in[m]}B_{i}$ , where $B_{i}=[s_i]^0$, $i\in[m]$. For a given $\bm{b}$, let $i\in B_j$ be its j-th entry and let $b(j,i)=0$ if $j\neq i$, $b(j,i)=1$ if $j=i$ and $b(0,i)=1$, for $i\in[m]$ and $j\in[s_i]$. Then if we define $\lambda_{0i}(\bm{\theta}_i,\bm{d})=1$, for every $\bm{\theta}_i\in \bm{\Theta}_i$, $\bm{d}\in\bm{\mathcal{D}}$ and $i\in[m]$, we can write 
\begin{eqnarray}
f_{\bm{d}}\left( \bm{\lambda }_{_{1}}(\bm{\theta }_{1},\bm{d}),\dots,\bm{\lambda }_{m}(\bm{\theta }_{m},\bm{d})\right)  &= &\sum_{\bm{b}\in B}k(\bm{b},\bm{d})\lambda _{\bm{b}}(\bm{ \theta },\bm{d})  \label{algebraic utility} \\
\text{where }\lambda _{\bm{b}}(\bm{\theta },\bm{d}) &= &\prod_{i\in[m]}\prod_{j\in[s_i]^0}\lambda_{ji}(\bm{\theta }_{i},\bm{d})^{b(j,i)}  \label{eq:aceu2}
\end{eqnarray}%
is a monomial, having at most one term not unity delivered by each panel. So in particular $f_{\bm{d}}( \cdot) $ is a square free polynomial. Let, for a given $\bm{b}\in B$, 
\begin{equation*}
\mu _{ji}(\bm{d})= \mathbb{E}\left( \lambda_{ji}(\bm{\theta }_{i},\bm{d})^{b(j,i)}\right). 
\end{equation*}

For the distributivity of the \gls{IDSS} we need the following property.
\begin{definition}
Call an \gls{IDSS} \textbf{score separable} if, in the notation above, the collective agrees that for all decisions $\bm{d}\in \bm{\mathcal{D}}$ and all indices $\bm{b}\in B$ such that $k(\bm{b},\bm{d})\neq 0$
\begin{equation}
\mathbb{E}\left( \lambda _{\bm{b}}(\bm{\theta },\bm{d})\right)=\prod_{i\in[m]}\prod_{j\in[s_i]^0}\mu_{ji}(\bm{d}) \label{uncor}
\end{equation}
\label{def:scoresep}
\end{definition}
 Let, for every $\bm{d}\in\bm{\mathcal{D}}$, $\bm{\mu }_{i}(\bm{d})^\T=\left( \mu _{ji}(\bm{d})\right)_{j\in[s_i]}$. A consequence of the definitions above is the following. 

\begin{lemma}
\label{lemma:ciao}
Suppose panel $G_{i}$ delivers its vector of expectations $\bm{\mu }_{i}(\bm{d})$, $i\in[m]$, to the \gls{SB}. Then, under the collective assumptions of structural consensus and an algebraic conditional expected utility, if the \gls{IDSS} is score separable then it is  adequate.
\end{lemma}
\begin{proof}
This follows immediately from equations (\ref{algebraic utility})-(\ref{uncor}), the definitions of the arguments in these functions and the definition of adequacy in Definition \ref{def:adequacy}.
\end{proof}

From Lemma \ref{lemma:ciao} we therefore deduce that adequacy is guaranteed whenever score separability holds, under the assumption of an algebraic conditional expected utility. In the following section we introduce conditions that assure this type of separability. We then identify classes of models that give rise to algebraic conditional expected utilities.

\section{Moment and Partial Independence}
\label{sec:momind}
Equation (\ref{uncor}) together with Lemma \ref{lemma:ciao} shows that adequacy is guaranteed whenever the expectation of certain functions of the panels' parameters separate appropriately. We introduce now a new type of independence termed partial independence.
\begin{definition}
\label{def:partial}
Let $f_{\bm{d}}(\bm{\lambda }_{_{1}}(\bm{\theta }_{1},\bm{d}),\dots,\bm{\lambda }_{m}(\bm{\theta }_{m},\bm{d}))$ be the algebraic conditional expected utility of an \gls{IDSS}. We say that the \gls{IDSS} is \textbf{partially independent} if 
\begin{equation*}
\E(f_{\bm{d}}(\bm{\lambda }_{_{1}}(\bm{\theta }_{1},\bm{d}),\dots,\bm{\lambda }_{m}(\bm{\theta }_{m},\bm{d})))=f_{\bm{d}}(\E(\bm{\lambda }_{_{1}}(\bm{\theta }_{1},\bm{d})),\dots,\E(\bm{\lambda }_{m}(\bm{\theta }_{m},\bm{d}))).
\end{equation*}
\end{definition}
This condition requires the expectation of the product of certain functions of the parameters overseen by different panels to be equal to the product of the individual expectations.

Often the $\lambda_{ji}$, $i\in[m]$, $j\in[s_i]$, are monomial functions of the panels' parameters. For this reason it is helpful to introduce the following independence condition. Let $<_{lex}$ denote a lexicographic order (see Appendix \ref{appendixD}).

\begin{definition}
\label{def:moment}
Let $\bm{\theta}^\T=(\theta_i)_{i\in[n]}\in\mathbb{R}^{n}$ be a parameter vector and $\bm{b}^\T=(b_i)_{i\in[n]}\in\mathbb{Z}^n_{\geq 0}$. We say that $\bm{\theta}$ entertain \textbf{moment independence} of degree $\bm{b}$ if for any $\bm{b}'=\left(b_i'\right)_{i\in[n]}^\T<_{lex} \bm{b}$ 
\begin{equation*}
\E\left(\bm{\theta}^{b'}\right)=\prod_{i\in[n]}\E\left(\theta_i^{b_i'}\right).
\end{equation*}
\end{definition}

It is generally well known that standard probabilistic independence only guarantees that the first moment of a product can be written as the product of the moments. Separations for higher orders are implied by standard independence only through a cumulant parametrization, where the cumulant generating function for a product of independent random variables (defined as a random sum of independent realizations) is the composition of the respective cumulant generating functions.

For the purposes of decision support in partial belief systems it is helpful to study moments, since expected utilities often formally depend on these. Now consider for instance two parameters $\theta_1$ and $\theta_2$. Assume a conditional expected utility is equal to $\theta_1^2\theta_2^2$ and that a conditional independence of degree $(2,2)$ holds. Then
\[
\mathbb{E}\left(\theta_1^2\theta_2^2\right)=\mathbb{E}\left(\theta_1^2\right)\mathbb{E}\left(\theta_2^2\right)=\mathbb{E}(\theta_1)^2\mathbb{E}(\theta_2)^2+\mathbb{E}(\theta_1)^2\mathbb{V}(\theta_2)+\mathbb{E}(\theta_2)^2\mathbb{V}(\theta_1)+\mathbb{V}(\theta_1)\mathbb{V}(\theta_2).
\]
The same expression is obtained when using sequentially the tower rule of expectations and the law of total variance of Proposition \ref{prop:towerrules} under the assumption of independence of the two parameters. Therefore the expression obtained under moment independence is reasonable and coincides with the one under standard independence, but it formally does not require it.  

\section{Adequacy in Partially Defined Systems}
\label{sec:adequacy1}

 Given the definitions of new independence concepts tailored for \glspl{IDSS}, we can now study in which cases adequacy holds. The following result easily follows from the definition of partial independence.
 
\begin{lemma}
\label{lemma:ciaociao}
Suppose $f_{\bm{d}}(\bm{\lambda }_{_{1}}(\bm{\theta }_{1},\bm{d}),\dots,\bm{\lambda }_{m}(\bm{\theta }_{m},\bm{d}))$ is the algebraic conditional expected utility of an \gls{IDSS} and that partial independence is in the structural consensus. The \gls{IDSS} is then adequate if panel $G_i$ delivers the vector of expectations $\bm{u}_i(\bm{d})$, $i\in[m]$.
\end{lemma}
\begin{proof}
This result easily follows from Definition \ref{def:scoresep} and \ref{def:partial} and Lemma \ref{lemma:ciao}
\end{proof}

Conversely, assuming the conditional expected utility is a polynomial in the panels' parameters, under a specific moment independence assumption we have the following. 

\begin{lemma}
\label{lemma:ciao3}
Assume $f_{\bm{d}}(\bm{\lambda }_{_{1}}(\bm{\theta }_{1},\bm{d}),\dots,\bm{\lambda }_{m}(\bm{\theta }_{m},\bm{d}))$ is the algebraic conditional expected utility of an \gls{IDSS}, $\bm{\theta}_i^\T=(\theta_{ji})_{j\in[s_i]}$ and $\lambda_{ji}=\bm{\theta}_{i}^{\bm{a}_{ji}}$, with $\bm{a}_{ji}\in\mathbb{Z}_{\geq 0}^{s_i}$, $i\in[m]$, $j\in[s_i]$. Let $\bm{a}^*_i$ be the highest $\bm{a}_{ji}$, $j\in[s_i]$ according to a lexicographic order, $i\in[m]$, and let $\bm{a}^*=\left(\bm{a}_i^*\right)_{i\in[m]}^\T$. Calling $\bm{\theta}^\T=(\bm{\theta}_i^\T)_{i\in[m]}$, if the structural consensus includes the assumption that $\bm{\theta}$ entertains moment independence of degree $\bm{a}^*$, then the \gls{IDSS} is adequate if panel $G_i$ delivers  the vector of expectations $\bm{u}_i(\bm{d})$, $i\in[m]$.
\end{lemma} 
\begin{proof}
This result is an easy consequence of the definition of score separability, moment independence and the equations (\ref{algebraic utility}) and (\ref{eq:aceu2}).
\end{proof}

Both Lemma \ref{lemma:ciaociao} and \ref{lemma:ciao3} start with the assumption of an algebraic conditional expected utility. For many of the examples we consider in Section \ref{sec:parexamples} this is the case. However we are able to identify families of utility factorizations and statistical models that assure the associated conditional expected utility is algebraic.  We first introduce a family of utilities that factorize according to the construction of the panels.
\begin{definition}
Let $\bm{Y}_i$ be the vector overseen by panel $G_i$, $i\in[m]$, and $\bm{R}_i$ be a function of $\bm{Y}_i$ and $\bm{d}\in\bm{\mathcal{D}}$ only. A multilinear factorization over $\bm{R}_1,\dots,\bm{R}_m$ is called \textbf{panel separable}.   
\end{definition}
The class of compatible utility factorizations in Definition \ref{compatible} can be seen to be in the family of panel separable utilities. 

The probabilistic model class we consider here is a generalization of the linear regressions in equation (\ref{eq:GausBN}) to generic polynomial functions, that we term henceforth \textit{polynomial \gls{SEM}} \citep[see e.g.][]{Wall2000}. \glspl{SEM} are widely used especially in the causal literature \citep{Pearl2000}. 

\begin{definition}
\label{def:polystrut}
Let $\bm{Y}^\T=(Y_i)_{i\in[n]}$ be a random vector. A polynomial structural equation model is defined by, for $i\in[n]_1$,
\begin{equation*}
Y_i=\sum_{\bm{b}_i\in B_i}\theta_{i\bm{b}_i}\left(Y_{[i-1]}\right)^{\bm{b_i}}+\varepsilon_i,
\end{equation*}
where $B_i\subset \mathbb{Z}^{i-1}_{\geq 0}$, $\varepsilon_i$ is a mean zero error and $\theta_{i\bm{b}_i}$ is a parameter, $i\in[n]$, $\bm{b}_i\in B_i$.
\end{definition} 
Note that these models are suitable candidates for a \gls{CK-class} since their definition is qualitative in nature.
 
For polynomial \glspl{SEM} and panel separable utilities, the following holds.
\begin{theorem}
\label{theo:ciao}
Suppose $\bm{R}=\bm{Y}$, that the utility consensus of an \gls{IDSS} includes a panel separable utility and the structural consensus includes a polynomial \gls{SEM}. Further suppose each panel agreed to model its marginal utility as a polynomial utility function. Then if panels are partially independent, the \gls{IDSS} is score separable.
\end{theorem}
\begin{proof}
This follows immediately by noting that under the assumptions of polynomial panel separable utility and polynomial structural equations, the conditional expected utility is algebraic. Partial independence then guarantees score separability. 
\end{proof}

Note that the partial independence condition of Theorem \ref{theo:ciao} actually corresponds by construction to a moment independence. The degree of such independence depends on the polynomial form of both the structural equation model and the utility function. In Section \ref{sec:bn1} we  identify the moment independence condition required for adequacy within a subclass of polynomial \glspl{SEM}.

Although we have not been focusing on inferential issues in this chapter, it is important to note that the property of score separability is  conjugate with the class of panel separable likelihoods of Chapter \ref{chapter3}.
\begin{proposition}
If an \gls{IDSS} is score separable a priori, then it will be so a posteriori if data admitted into the system comes from either panel separable or e-panel compatible likelihoods.
\end{proposition}
\begin{proof}
This easily follows from Definitions \ref{def:liksep} and \ref{def:scoresep}.
\end{proof}

Importantly, the algebraic approach developed here for \glspl{IDSS} allows us to identify target functions of the panels' parameters the expected utility formally depends on. Therefore Bayesian probabilistic updating does not need to be performed over full distribution but only with the purpose of having better estimates of this target functions. \citet{Bissiri2013} introduced a novel method to perform Bayesian updating only over some summaries of an unknown distribution, what we have called here a partial belief system. The use of such methods within \glspl{IDSS} is the subject of ongoing research. Furthermore, as we discuss in more length in Chapter \ref{chapter6}, when the conditional expected utility is a polynomial function of the moments of $\bm{Y}$, the updating could be performed in a Bayes linear fashion through linear fitting.

\section{Examples}
\label{sec:parexamples}
\subsection{Discrete Models}
\subsubsection{Independence Binary Models.}
We begin with a rather obvious setting where a small number of summaries are sufficient to determine an expected utility maximizing decision. Let the \gls{CK-class} specify that $\bm{R}=\bm{Y}=(Y_i)_{i\in[n]}^\T$, where each variable $Y_i$ is binary and overseen by panel $G_i$. Assume the \gls{CK-class} includes the belief that $\independent_{i\in[n]} Y_i\;|\; \bm{\theta}(\bm{d})$, where $\bm{\theta}(\bm{d})=(\theta_i(\bm{d}))_{i\in[n]}^\T$ and $\theta_i(\bm{d})=P(Y_i=1\;|\;\bm{d})$, whatever decision $\bm{d}\in\bm{\mathcal{D}}$ is made, $i\in[n]$. We have already discussed in Section \ref{sec:indadd} how independence models can be part of the structural consensus of a \gls{CK-class}. Similarly to the example in Section \ref{sec:adequacy}, suppose each panel $G_i$ delivers the set of beta distributions $Be(p_i(\bm{d}),q_i(\bm{d}))$ for $\bm{\theta}_i(\bm{d})$. 

Assume further the \gls{CK-class} includes utility factorizations of the form
\begin{equation*}
\label{eq:ind1}
u(y_1,\dots, y_n)=\sum_{i\in[n]}k_iy_i+\sum_{i\in[n]}\sum_{j\in[n_i]}k_{ij}y_iy_j.
\end{equation*}
With no further assumptions, the conditional expected utility can be written as
\begin{equation}
\label{eq:ind2}
\bar{u}(\bm{d}\;|\;\bm{\theta})=\sum_{i\in[n]}k_i\theta_{i}(\bm{d})+\sum_{i\in[n]}\sum_{j=i+1}^nk_{ij}\theta_i(\bm{d})\theta_j(\bm{d})
\end{equation}

Equation (\ref{eq:ind2}) corresponds to an algebraic conditional expected utility, whose $\lambda_i(\theta_i,\bm{d})$ are univariate and coincide with  $\theta_i(\bm{d})$, $i\in[n]$. Therefore in this example  partial independence corresponds to moment independence of degree $\bm{1}$, where $\bm{1}$ is a vector of dimension $n$ with $1$ in all its entries. In this case thus, partial independence is implied by standard independence of the parameter vector. Furthermore all the monomials $\lambda_{\bm{a}}(\bm{\theta},d)$ formally defined in equation (\ref{algebraic utility}) here are monomials of degree  either  one or two corresponding respectively to $\lambda_i(\theta_i,\bm{d})$ or $\lambda_i(\theta_i,\bm{d})\lambda_j(\theta_j,\bm{d})$, for $i\in[n]$ and $j\in[n]_i$.

Now let $\mu_i(\bm{d})=p_i(\bm{d})(p_i(\bm{d})+q_i(\bm{d}))^{-1}=\mathbb{E}(\theta_i(\bm{d}))$ and assume partial independence is in the \gls{CK-class}. It then easily follows that
\begin{equation*}
\label{eq:ind3}
\bar{u}(\bm{d})=\sum_{i\in[n]}k_i\mu_i(\bm{d})+\sum_{i\in[n]}\sum_{j\in[n]_i}k_{ij}\mu_i(\bm{d})\mu_j(\bm{d}),
\end{equation*}
guaranteeing that the \gls{IDSS} is adequate. 


\subsubsection{Staged Trees.}
Consider now the staged tree in Figure \ref{fig:ET} of Section \ref{sec:tree}, that we report again on this page for convenience. As discussed in Section \ref{sec:IDSSET}, staged trees can be part of a coherent \gls{IDSS} whenever panels oversee disjoint subsets of either the position set or the stage set. Recall that this tree has four stages (coinciding with its positions) $w_0=\{v_0\},$ $w_1=\{v_1\}$, $w_2=\{v_2\}$ and $w_3=\{v_3,v_4\}$. Suppose there are three panels $G_1$, $G_2$ and $G_3$ having responsibility over $w_0$, $\{w_1,w_2\}$ and $w_3$ respectively.

\begin{figure}
\centerline{
\hspace*{-16mm} \xymatrixrowsep{0.5pc}{\xymatrixcolsep{2pc}
\xymatrix{
&&&&\bullet~v_{7}\\
&&&\bullet~v_{3}\ar@[Gruen1][r]_{\text{no}}\ar@[Gruen0][ru]^{\text{yes}}&\bullet~v_{8}\\
&&\bullet~v_{1}\ar[r]_{\text{low}}\ar[ru]^{\text{high}}&\bullet~v_{4}\ar@[Gruen0][r]|{\text{yes}}\ar@[Gruen1][rd]_{\text{no}}&\bullet~v_{9}\\
v_0\hspace*{-16mm}&\bullet\ar[ru]^{\text{yes}}\ar[rd]_{\text{no}}&&&\bullet~v_{10}\\
&&\bullet~v_{2}\ar[r]^{\text{high}}\ar[rd]_{\text{low}}&\bullet~v_{5}\\
&&&\bullet~v_{6}
}}}
\end{figure}

In the notation of Example \ref{ex:staged}, assume the collective has agreed on an additive utility factorization such that 
\[
u(y_1,y_2,y_3)=k_1u_1(y_1)+k_2u_2(y_2)+k_3u_3(y_3),
\]
where $Y_1,Y_2,Y_3$ are the variables in the associated \gls{BN} representation of this tree, and that they have been jointly able to further specify the criterion weights. Leaving the dependence on the decisions $\bm{d}$ implicit, let $\psi_i(\sigma)=u_i(\sigma)$, where $\sigma\in\{0,1\}$, and recall that $\theta_{ss'}=\mathbb{P}(Y_s=s')$, for a situation $s\in S(\mathcal{T})$ and $s'\in\mathcal{Y}_s=\{1,0\}$. Label the outcomes of this tree with a $1$ for yes and high, whilst a $0$ denotes no and low outcomes. In this parametrization, the staged tree in Figure \ref{fig:ET} introduces the constraints $\theta_{31}=\theta_{41}$ and $\theta_{30}=\theta_{40}$, where for example $\theta_{31}=\mathbb{P}(Y_3=yes\;|\; Y_2=high, Y_1=yes)$.

Through a sequential application of the tower rule of expectation, it can be easily deduced that the conditional expected utility of this problem can be written as
\begin{equation}
\label{eq:staged2}
\bar{u}(\bm{\theta}\;|\;\bm{d})=\bar{u}_1+\bar{u}_2+\bar{u}_{3}
\end{equation}
where
\begin{align}
\begin{split}
\label{eq:staged3}
&\bar{u}_{3}=k_3\psi_{31}\theta_{31}\theta_{11}\theta_{01}+k_3\psi_{30}\theta_{30}\theta_{11}\theta_{01}+k_3\psi_{31}\theta_{41}\theta_{10}\theta_{01}+k_3\psi_{30}\theta_{40}\theta_{10}\theta_{01}\\
&\bar{u}_2=k_2\psi_{21}\theta_{11}\theta_{01}+k_2\psi_{20}\theta_{10}\theta_{01}+k_2\psi_{21}\theta_{21}\theta_{00}+k_2\psi_{20}\theta_{20}\theta_{00}\\
&\bar{u}_1=k_1\psi_{11}\theta_{01}+k_1\psi_{10}\theta_{00}.
\end{split}
\end{align}

The conditional expected utility in equations (\ref{eq:staged2})-(\ref{eq:staged3}) is again algebraic. The coefficients $k(\bm{a},\bm{d})$ of the monomials in equation (\ref{algebraic utility}) correspond to the jointly agreed criterion weights, and the unknown functions $\bm{\lambda}_{i}(\bm{\theta}_i,\bm{d})$ are as follows 
\begin{align*}
\bm{\lambda}_3(\bm{\theta}_3,\bm{d})&=(\psi_{31}\theta_{31},\psi_{30}\theta_{30})^\T,\\
\bm{\lambda}_2(\bm{\theta}_2,\bm{d})&=(\theta_{11},\theta_{10},\theta_{21},\theta_{20},\psi_{21}\theta_{11},\psi_{20}\theta_{10},\psi_{21}\theta_{21},\psi_{20}\theta_{20})^\T,\\
\bm{\lambda}_1(\bm{\theta}_1,\bm{d})&=(\psi_{11}\theta_{01},\psi_{10}\theta_{00})^\T.
\end{align*}
 So again these polynomials are extremely regular and a simple multilinear function of probabilities delivered by different panels. Under partial independence, as guaranteed by  Lemma \ref{lemma:ciao}, an \gls{IDSS} so defined is adequate. 
 
 Although under a different parametrization, in Section \ref{sec:symbolic} we introduce an algorithm to automatically compute expected utilities of the embedded symmetric problem associated to a staged tree. We also briefly characterize the asymmetries described for example by a staged tree. In section \ref{sec:diff} we explore in more details the polynomial characteristics of staged tree models. 

\subsection{Bayesian Networks}
\label{sec:bn1}

We now consider the \gls{BN} model class, where each variable of the network is defined as the linear regression introduced in equation (\ref{eq:GausBN}). Note that this is an instance of the polynomial \gls{SEM} of Definition \ref{def:polystrut} and we refer to this model as linear \gls{SEM} over a \gls{DAG} $\Gr$.  Although such model definition is often associated to a multivariate Gaussian distribution, this does not have to be the case in general.

 Similarly to \citet{Sullivant2008}, we consider regression parameters as indeterminates in a polynomial function and we associate these to edges and vertices of the underlying \gls{DAG}. Fix a decision $\bm{d}\in\bm{\mathcal{D}}$ and for ease of notation suppress this index. Let $\Gr$ be a \gls{DAG} with vertex set $V(\Gr)=\{Y_i:i\in[n]\}$ and let, for $i\in[n]$, $\theta_{0i}'=\theta_{0i}+\varepsilon_{i}$, be the indeterminate associated to the vertex $Y_i$, whilst $\theta_{ij}$, for $(Y_i,Y_j)\in E(\Gr)$, is the indeterminate associated to the corresponding edge.\footnote{We consider $\theta_{0i}'$ as a parameter although this consists of the sum of a parameter and an error. Note however that from a Bayesian viewpoint these are both random variables.} Define $\vec{P}_i$ as the set of rooted directed paths (see Appendix \ref{appendixB}) in $\Gr$ ending in $Y_i$. For every element $P\in\vec{P}_i$ we define $\bm{\theta}_P$ as
 \begin{equation*}
 \bm{\theta}_P=\prod_{Y_i\in P}\theta_{0i}'\prod_{(Y_i,Y_j)\in P}\theta_{ij},
 \end{equation*}
and we call it the \textit{path monomial}.

\begin{example}
Consider the \gls{DAG} in Figure \ref{fig:BNexample}. The set $\vec{P}_3$ for instance is equal to
\begin{equation}
\label{eq:paths}
\{(Y_3),(Y_2,(Y_2,Y_3)), (Y_1,(Y_1,Y_3)), (Y_1,(Y_1,Y_2),(Y_2,Y_3))\},
\end{equation}
and $\theta_{03}'$, $\theta_{02}'\theta_{23}$, $\theta_{01}'\theta_{13}$ and $\theta_{01}'\theta_{12}\theta_{23}$ are the corresponding path monomials.
\end{example}

For the purposes of this section we call \textit{algebraic substitution} the process of plugging-in the linear regression definition of a random variable of the \gls{DAG} into the structural equation definition of the parent variable. To illustrate this process, we present now an example of what we mean by algebraic substitution. 

\begin{example}
Consider again the \gls{DAG} in Figure \ref{fig:BNexample}. For this network the variables are defined as
\begin{equation*}
\begin{array}{ll}
Y_4=\theta_{04}+\theta_{14}Y_1+\varepsilon_1, & Y_3=\theta_{03}+\theta_{13}Y_1+\theta_{23}Y_2+\varepsilon_3,\\
Y_2=\theta_{02}+\theta_{12}Y_1+\varepsilon_2,& Y_1=\theta_{01}+\varepsilon_1.
\end{array}
\end{equation*}
An algebraic substitution of the variables in the definition of $Y_3$ entails
\begin{align}
Y_3&=\theta_{03}+\theta_{13}(\theta_{01}+\varepsilon_1)+\theta_{23}(\theta_{02}+\theta_{12}Y_1+\varepsilon_2)+\varepsilon_3\nonumber\\
&=\theta_{03'}+\theta_{13}\theta_{01}'+\theta_{23}\theta_{02}'+\theta_{23}\theta_{12}Y_1.\label{eq:y3}
\end{align}
Note that an additional algebraic substitution can be performed in equation (\ref{eq:y3}) so that 
\begin{equation}
Y_3=\theta_{03}'+\theta_{13}\theta_{01}'+\theta_{23}\theta_{02}'+\theta_{23}\theta_{12}\theta_{01}'.\label{eq:y3'}
\end{equation}
\end{example}
Note that importantly in equation (\ref{eq:y3'}) $Y_3$ is now defined only in terms of the parameters. The following proposition formalizes that this is in general the case for any variable of a \gls{DAG} defined as a linear \gls{SEM}.

\begin{proposition}
\label{prop:algsub}
Let $\Gr$ be a \gls{DAG} with vertex set $\{Y_i\}_{i\in[n]}$, whose variables are defined as a linear \gls{SEM}. Then through algebraic substitutions each variable $Y_i$, $i\in[n]$, can be written as
\begin{equation*}
Y_i=\sum_{P\in\vec{P}_i}\bm{\theta}_{P}.
\end{equation*}
\end{proposition}
The proof of this result is in Appendix \ref{sec:proofalg}.
\begin{example}
Equation \ref{eq:y3'} shows that $Y_3$ can be written as the sum of the monomials associated to each rooted path ending in $Y_3$, where the set of rooted paths is in equation (\ref{eq:paths}).
\end{example}

\subsubsection{Additive factorizations.}
Given Proposition \ref{prop:algsub}, we are now able to polynomially compute the expected utility of additive utility factorizations over the graphs in the case the marginal utility functions are polynomial.

\begin{lemma}
\label{lemma:addeu}
For a linear \gls{SEM} over a \gls{DAG} $\Gr$ with vertex set $\{Y_i\}_{i\in[n]}$, suppose $\bm{R}=\bm{Y}$ and 
\begin{equation*}
u(\bm{r})=\sum_{i\in[n]}k_iu_i(y_i).
\end{equation*}
Suppose further $u_i$ is a polynomial utility function of degree $n_i$. Then the conditional expected utility is algebraic and can be written as
\begin{equation}
\label{eq:addeu}
\bar{u}(\bm{d}\;|\;\bm{\theta})=\sum_{i\in[n]}k_i\sum_{j\in[n_i]^0}\rho_{ij}\sum_{|\bm{\alpha}_i|=j}\binom{j}{\bm{\alpha}_i}\bm{\theta}_{\Gr_i}^{\bm{\alpha}_i},
\end{equation}
where $\bm{\alpha}_i\in\mathbb{Z}_{\geq 0}^{\#\vec{P}_i}$, $\bm{\theta}_{\Gr_i}=\prod_{P\in\vec{P}_i}\bm{\theta}_P$, $\binom{j}{\bm{\alpha}_i}$ is a multinomial coefficient (see Appendix \ref{sec:multidir}) and $\#\vec{P}_i$ is the number of elements in $\vec{P}_i$.
\end{lemma}

\begin{proof}
From Proposition \ref{prop:algsub}, it follows that
\begin{equation*}
\label{eq:additiveEU}
u(\bm{y})=\sum_{i\in[n]}k_i\sum_{j\in[n_i]^0}\left(\sum_{P\in\vec{P}_i}\bm{\theta}_P\right)^j.
\end{equation*}
Applying the multinomial theorem, the result follows.
\end{proof}

Equation (\ref{eq:addeu}) is an instance of the computation of the moments of a decomposable function as studied in \citet{Cowell1999a} and \citet{Nilsson2001}, where in this case we explicitly deduce the required monomials and their degree. 

The proof of Lemma \ref{lemma:addeu} provides an intuitive graphical description of the required monomials for the computation of the expected utility in equation (\ref{eq:addeu}). Furthermore the proof instructs us on how to compute any marginal moment of a linear \gls{SEM}. Note that the $j$-th non central moment of any variable $Y_i$ in such a \gls{BN} can be written, via the multinomial theorem, as the sum of the monomials $\bm{\theta}_{\Gr_i}$ with degree $j$. We can observe that, following from the properties of multinomial coefficients, this sum can be thought of as the sum over the set of unordered $j$-tuples of paths ending in $Y_i$. Call this set $\vec{P}^j_i$. For a $j$-tuple $P\in\vec{P}^j_i$, the binomial coefficient in equation (\ref{eq:addeu}) counts the distinct permutations of the elements of $P$, denoted as $n_P$. We therefore have that
\begin{equation*}
\label{eq:grapheu}
\sum_{|\bm{\alpha}_i|=j}\binom{j}{\bm{\alpha}_i}\E\left(\bm{\theta}_{\Gr_i}^{\bm{\alpha}_i}\right)=\sum_{P\in\vec{P}^j_i}n_P\E\left(\prod_{p\in P}\bm{\theta}_p\right).
\end{equation*} 
 Equation \ref{eq:grapheu} is an intuitive graphical interpretation of equation \ref{eq:addeu}.

\begin{example}
For simplicity we consider now the vertex $Y_4$ in the \gls{DAG} of Figure \ref{fig:BNexample}. The set $\vec{P}_4$ is equal to $\{(Y_4), (Y_1,(Y_1,Y_4))\}$. From equation \ref{eq:addeu}, $Y_4^2$ can be written as 
\begin{equation}
\label{eq:exbn1}
{\theta_{04}'}^2+{\theta_{01}'}^2\theta_{14}^2+2\theta_{01}'\theta_{14}\theta_{04}'.
\end{equation}
The polynomial above can be equally deduced by simply looking at the \gls{DAG}. Note that 
\begin{equation*}
\vec{P}_4^2=\{((Y_4),(Y_4)), ((Y_1,(Y_1,Y_4)),(Y_1,(Y_1,Y_4))), ((Y_4),(Y_1,(Y_1,Y_4)))\}.
\end{equation*}
The first and second monomial in equation (\ref{eq:exbn1}) correspond to the first and second element of $\vec{P}_4^2$ respectively, whilst the last elements of this set, having two distinct permutation of its elements, is associated to the third monomial in equation (\ref{eq:exbn1})
\end{example}

The above results instruct us on the required independences to be entertained by \glspl{BN} in \glspl{IDSS}.
\begin{theorem}
Suppose the structural consensus of an \gls{IDSS} includes a linear \gls{SEM} over a \gls{DAG} with vertex set $\{Y_i:i\in[n]\}$, where panel $G_i$ oversees the vertex $Y_i$ of $\Gr$, $i\in[n]$. Suppose the utility consensus consists of a  multilinear factorization and panel $G_i$ agreed to model its marginal utility with a polynomial utility function of degree $n_i\in\mathbb{Z}_{>0}$, $i\in[n]$. Let $l_P$ be the length of a rooted path $P$. If for every $i\in[n]$ and $P\in\vec{P}_i$, $\bm{\theta}_P$ entertains moment independence of degree $\bm{n}_{iP}=(n_i,\dots, n_{i})^\T\in\mathbb{Z}^{l_P}_{\geq 0}$ and $\bm{\theta}_{\Gr_i}$ entertains moment independence of degree $\bm{\alpha}_i$ for every $|\bm{\alpha}_i|=n_i$, where $\bm{\alpha}_i\in\mathbb{Z}_{\geq 0}^{l_i}$, $l_i=\sum_{P\in\vec{P}_i}l_P$, then, under the conditions of Lemma \ref{lemma:addeu}, the \gls{IDSS} is score separable.  
\end{theorem}
\begin{proof}
This easily follows from both Lemma \ref{lemma:addeu} and the definition of moment independence in Definition \ref{def:moment}. 
\end{proof}

We can observe that in this case the functions $\lambda_{ij}$ panel $G_i$ delivers, $i\in[n]$, are simply powers of monomials having one indeterminate only. Therefore, if panels deliver the expectation of these functions, where the required degrees are specified by Lemma \ref{lemma:addeu}, an \gls{IDSS} so defined will be adequate. 

\subsubsection{Multilinear factorizations.}
The algebraic approach we have taken in this chapter allows us to generalize the results on additive/decomposable functions to multilinear functions straightforwardly. We start analysing this more general case by stating two introductory results. 

\begin{proposition}
\label{prop:1}
Let $\Gr$ be a \gls{DAG} with vertex set $\{Y_i\}_{i\in[n]}$ and assume $\bm{R}=\bm{Y}$. Assume further a multilinear utility factorization over the graph such that
\begin{equation}
\label{eq:multidag}
u(\bm{r})=\sum_{I\in\mathcal{P}_0([n])}k_I\prod_{i\in I}u_i(y_i),
\end{equation} 
and let each marginal utility $u_i(y_i)$ be polynomial of degree $n_i$. Then, letting $\bm{n}=(n_i)_{i\in[n]}^\T\in\mathbb{Z}^n$,
\begin{equation}
\label{eq:multiutpol}
u(\bm{r})=\sum_{\bm{0}<_{lex}\bm{\alpha}\leq_{lex}\bm{n}}c_{\bm{\alpha}}\bm{y}^{\bm{\alpha}},
\end{equation}
where $\bm{\alpha}=(\alpha_i)_{i\in[n]}^\T\in\mathbb{Z}^{n}_{\geq 0}$, $c_{\bm{\alpha}}=k_I\prod_{i\in I}\rho_{i\alpha_i}$ and $I=\{i\in[n]: \alpha_i\neq 0\}$. 
\end{proposition}
\begin{proof}
This result easily follows noting that the product of complete polynomials (meaning having all degrees - except for zero - lower than the degree of the polynomial) is complete.
\end{proof}

\begin{proposition}
\label{prop:2}
For a linear \gls{SEM} over $\Gr$ with vertex set $\{Y_i:i\in[n]\}$, letting $\bm{\alpha}=(\alpha_i)_{i\in[n]}^\T\in\mathbb{Z}^{n}_{\geq 0}$ and assuming $\#\vec{P}_i=n_i$, $i\in[n]$, we have that 
\begin{equation}
\label{eq:multiciao}
\bm{Y}^{\bm{\alpha}}=\sum_{\bm{\beta}\simeq \bm{\alpha}}\binom{|\bm{\alpha}|}{\bm{\beta}}\bm{\theta}_{\Gr}^{\bm{\alpha}},
\end{equation}
where $\bm{\beta}^\T=(\bm{\beta}_i^\T)_{i\in[n]}$, $\bm{\beta}_i\in\mathbb{Z}^{n_i}_{\geq 0}$, $\bm{\theta}_{\Gr}=\prod_{i\in[n]}\bm{\theta}_{\Gr_i}$ and we say $\bm{\alpha}\simeq \bm{\beta}$ if both $|\bm{\alpha}|=|\bm{\beta}|$ and $\forall ~ i\in[n]$ $|\bm{\beta}_i|=\alpha_i$.  
\end{proposition}

\begin{proof} The monomial $\bm{Y}^{\bm{\alpha}}$ can be written as
\begin{align*}
\bm{Y}^{\bm{\alpha}}&=\prod_{i\in[n]}Y_i^{\alpha_i}=\prod_{i\in[n]}\left(\sum_{|\bm{\beta}_i|=\alpha_1}\binom{\alpha_i}{\bm{\beta}_i}\bm{\theta}_{\Gr_i}^{\bm{\beta}_i}\right)=\sum_{\bm{\beta}\sim \bm{\alpha}}\prod_{i\in[n]}\binom{\alpha_i}{\bm{\beta_i}}\bm{\theta}_{\Gr_i}^{\bm{\beta}_i}.
\end{align*}
The above expression can then be seen to be equal to equation (\ref{eq:multiciao}) by using the properties of multinomial coefficients. 
\end{proof}

From the above two results we then have the following.
\begin{lemma}
\label{lemma:multieu}
Under the assumptions of Proposition \ref{prop:1} and \ref{prop:2}, the conditional expected utility is algebraic and can be written as
\begin{equation}
\label{eq:multieu}
\E(\bm{d}\;|\;\bm{\theta})=\sum_{\bm{0}<_{lex}\bm{\alpha}\leq_{lex}\bm{n}}c_{\bm{\alpha}}\sum_{\bm{\beta}\simeq \bm{\alpha}}\binom{|\bm{\alpha}|}{\bm{\beta}}\E\left(\bm{\theta}_{\Gr}^{\bm{\alpha}}\right).
\end{equation}
\end{lemma}

\begin{proof}
This follows by plugging-in equation (\ref{eq:multiciao}) into equation (\ref{eq:multiutpol}) and then using the properties of expectations.
\end{proof}

Importantly Lemma \ref{lemma:multieu} generalized the theory of the computation of moments in decomposable/additive functions of \citet{Cowell1999a} and \citet{Nilsson2001} to multilinear functions of \glspl{BN} defined through a linear \gls{SEM}. This result is also connected to the propagation algorithms developed in \citet{Lauritzen1992}. In that paper, the computation of the first two moments of a much larger class models (chain graphs with both discrete and continuous variables) is considered. Here, focusing only on a certain class of continuous \gls{DAG} models, we are able to explicitly compute, through algebraic substitution, any moment of the distribution associated to the graph.  

Using again the properties of multinomial coefficients, we can relate  equation (\ref{eq:multieu}) to the topology of the graph and its rooted paths. For an $\bm{\alpha}\in\mathbb{Z}^{n}_{\geq 0}$, let $\vec{P}^{\bm{\alpha}}$ be the set of unordered $|\bm{\alpha}|$-tuples of paths, where in each tuple there are $\alpha_i$ paths ending in $Y_i$. For each element $P\in\vec{P}^{\bm{\alpha}}$, let $n_P$ be the sum of the distinct permutations of the paths ending in $Y_i$. Then we have that
 \begin{equation*}
 \sum_{\bm{\beta}\simeq \bm{\alpha}}\binom{|\bm{\alpha}|}{\bm{\beta}}\E\left(\bm{\theta}_{\Gr}^{\bm{\alpha}}\right)=\sum_{P\in\vec{P}^{\bm{\alpha}}}n_P\E\left(\prod_{p\in P}\bm{\theta}_p\right)
 \end{equation*}
 \begin{example}
 Consider $\E(Y_2^2Y_4^2)$. All distinct tuples of dimension four where two paths end in $Y_2$ and two  in $Y_4$ are summarized in Table \ref{table:tuples}. The associated conditional expectation can be written as the following polynomials, where the $i$-th monomial corresponds to the tuple in the $i$-th row of Table $\ref{table:tuples}$.
 \begin{multline*}
 {\theta_{02}'}^2{\theta_{04}'}^2+
 2\theta_{12}{\theta_{02}'}{\theta_{04}'}^2+
 \theta_{12}^2{\theta_{04}'}^2+
 2{\theta_{02}'}^2\theta_{14}\theta_{04}'+
 4\theta_{12}\theta_{02}'\theta_{14}\theta_{04}+\\
 2\theta_{12}^2\theta_{14}\theta_{04}'+
 {\theta_{02}'}^2\theta_{14}^2+
 2\theta_{12}\theta_{02}'\theta_{14}^2+
 \theta_{12}^2\theta_{14}^2.
 \end{multline*}
 Note for example that $\theta_{12}{\theta_{02}'}{\theta_{04}'}^2$ has coefficient 2 since the paths $(Y_2)$ and $(Y_1,(Y_1,Y_2))$ can be permuted, whilst $\theta_{12}\theta_{02}'\theta_{14}\theta_{04}$ has coefficient 4 since both pairs of paths $(Y_2)$ and $(Y_1,(Y_1,Y_2))$ and $(Y_4)$ and $(Y_1,(Y_1,Y_4))$ can be permuted.
 \end{example}
 
 \begin{table}
 \renewcommand{\arraystretch}{1.5}
 \begin{center}
 \begin{tabular}{|c|}
 \hline
 $((Y_2),(Y_2),(Y_4),(Y_4))$\\
 $((Y_1,(Y_1,Y_2)),(Y_2),(Y_4),(Y_4))$\\
 $((Y_1,(Y_1,Y_2)),(Y_1,(Y_1,Y_2)),(Y_4),(Y_4))$\\
 $((Y_2),(Y_2),(Y_1,(Y_1,Y_4)),(Y_4))$\\
 $((Y_1,(Y_1,Y_2)),(Y_2),(Y_1,(Y_1,Y_4)),(Y_4))$\\
  $((Y_1,(Y_1,Y_2)),(Y_1,(Y_1,Y_2)),(Y_1,(Y_1,Y_4)),(Y_4))$\\
  $((Y_2),(Y_2),(Y_1,(Y_1,Y_4)),(Y_1,(Y_1,Y_4)))$\\
 $((Y_1,(Y_1,Y_2)),(Y_2),(Y_1,(Y_1,Y_4)),(Y_1,(Y_1,Y_4)))$\\
 $((Y_1,(Y_1,Y_2)),(Y_1,(Y_1,Y_2)),(Y_1,(Y_1,Y_4)),(Y_1,(Y_1,Y_4)))$\\
 \hline
 \end{tabular}
 \end{center}
 \caption{Tuples of dimension 4 with two paths ending in $Y_2$ and two more ending in $Y_4$ in the graph in Figure \ref{fig:BNexample}. \label{table:tuples}}
 \end{table}

Similarly to the additive case, we are now able to deduce the independences required for score separability of an \gls{IDSS} defined over a \gls{BN}.

\begin{theorem}
Suppose the structural consensus of an \gls{IDSS} includes a linear \gls{SEM} over a \gls{DAG} with vertex set $\{Y_i:i\in[n]\}$, where panel $G_i$ oversees the vertex $Y_i$ of $\Gr$, $i\in[n]$. Suppose the utility consensus consists of a panel separable multilinear factorization as in equation (\ref{eq:multidag}) and panel $G_i$ agreed to model its marginal utility with a polynomial utility function of degree $n_i\in\mathbb{Z}_{>0}$, $i\in[n]$. Let $l_P$ be the length of a rooted path $P$. If for every $i\in[n]$ and $P\in\vec{P}_i$, $\bm{\theta}_P$ entertains moment independence of degree $\bm{n}_{iP}=(n_i,\dots, n_{i})^\T\in\mathbb{Z}^{l_P}_{\geq 0}$ and $\bm{\theta}_{\Gr}$ entertains moment independence of degree $\bm{\alpha}$ for every $|\bm{\alpha}|=|\bm{n}|$, where $\bm{n}=(n_i)_{i\in[n]}^\T\in\mathbb{Z}^n_{\geq 0}$, $\bm{\alpha}^\T=(\bm{\alpha}_i^\T)_{i\in[n]}$ and $\bm{\alpha}_i\in\mathbb{Z}_{\geq 0}^{l_i}$, for $l_i=\sum_{P\in\vec{P}_i}l_P$, $i\in[n]$, then the \gls{IDSS} is score separable.  
\end{theorem}
 \begin{proof}
 This easily follows from both Lemma \ref{lemma:multieu} and the definition of moment independence in Definition \ref{def:moment}.
 \end{proof}
 
 We can observe that in the multilinear case, differently from the additive one, the functions $\lambda_{ij}$ panel $G_i$ delivers are now powers of monomials having one or more indeterminates. The degree of such powers are specified in Lemma \ref{lemma:addeu}.
  
\subsection{Conflict Models}
In this section we exploit the algebraic structure of the conditional expected utility of some simple \glspl{IDSS} to deduce which panels' belief specification  builds either agreement or disagreement between the members of the collective. 
 
\subsubsection{A first simple example.}
\label{sec:quadratic}
Suppose an \gls{IDSS} simply consists of two random variables, $Y_1$ and $Y_2$, assumed independent of each other and overseen by two different panels. For instance $Y_1$ might represent the costs deriving from a nuclear accident, whilst $Y_2$ is the number of people not exposed by the contamination. Suppose it has to be identified the optimal number  $d\in \mathbb{Z}_{0< d\leq D}$, $D\in\mathbb{Z}$, of inhabitants to be evacuated from a nearby village. Assume $Y_i\sim Exp(\theta_i/d)$, $\theta_i\in\mathbb{R}_{> 0}$, $i\in[2]$, meaning that $Y_i$ follows an exponential distribution,  and let
\begin{equation*}
u(y_1,y_2)=k_1u_1(y_1)+k_2u_2(y_2),
\end{equation*}
with $u_1(y_1,d)=-\alpha_1y_1^2-\beta_1y_1+\gamma_1$, $u_2(y_2,d)=\alpha_2y_2^2+\beta_2y_2$ and $\alpha_1,\alpha_2,\beta_1,\beta_2,\gamma_1\in\mathbb{R}_{\geq 0}$. Recall that $\E(Y_i)=d/\theta_i$ and $\V(Y_i)=d^2/\theta_i^2$. Therefore as the number of people evacuated increases, both the costs and the number of non exposed people increase as well. Furthermore note that $u_1$ and $u_2$ are respectively an increasing and decreasing function of their arguments in $\mathbb{R}_{\geq 0}$.

It can be easily deduced that the conditional expected utility for an \gls{IDSS} so defined is  equal to
\begin{equation*}
\bar{u}(d\;|\;\bm{\theta})=2\left(\frac{k_2\alpha_2}{\theta_2^2}-{k_1\alpha_1}{\theta_1^2}\right)d^2+\left(\frac{k_2\beta_2}{\theta_2}-{k_1\beta_1}{\theta_1}\right)d+k_1\gamma_1.
\end{equation*}   
This is a quadratic function of the decision $d$. Note that if 
\begin{equation*}
\frac{k_2\alpha_2}{\theta_2^2}>\frac{k_1\alpha_1}{\theta_1^2},
\end{equation*}
then $\bar{u}(d\;|\;\bm{\theta})$ is concave. Assuming $k_2\beta_2/\theta_2<k_1\beta_1/\theta_1$, otherwise the maximum of the function is at zero, $\bar{u}(d\;|\;\bm{\theta})$ is maximum for some point $d^*$ within the interval $(0,D]$. Therefore optimal policies will in general consist of a balancing of the two attributes and both panels are likely to agree on such a course of action. 

On the other hand when 
\begin{equation*}
\frac{k_2\alpha_2}{\theta_2^2}<\frac{k_1\alpha_1}{\theta_1^2},
\end{equation*}
then $\bar{u}(d\;|\;\bm{\theta})$ is convex. In this case then the maximum will either be at $D$ or approaching zero,  corresponding to only concerning about the cost or the exposition attributes respectively. In this situation it will be difficult for panels to agree on the outputs of the \gls{IDSS} and any policy choices might be contentious. Note that when the coefficient of $d^2$ is close to zero, then small changes in the elicitation of the parameters might lead to a change from one of the two cases described above to the other. 
  
\subsubsection{A more composite scenario.}
Although the previous example shed light on the information that the conditional expected utility function carries about the behaviour of its optimal, the study of this behaviour was based on simple analysis concept. We now consider a more difficult situation and use results from catastrophe theory \citep{Poston2014,Zeeman1979} in the context of expected utility maximization similarly to \citet{Dodd2012} and \citet{Smith2012}.

Suppose an \gls{IDSS} is defined as in Section \ref{sec:quadratic} above, with the only difference that the structural consensus now includes the utility factorization
\begin{equation*}
u(y_1,y_2)=u_1(y_1)u_2(y_2).
\end{equation*}  
The conditional expected utility can then be computed as
\begin{equation*}
\bar{u}(d\;|\;\bm{\theta})=-4\frac{\alpha_1\alpha_2}{\theta_1^2\theta_2^2}d^4-2\left(\frac{\alpha_1\beta_2\theta_2+\beta_1\alpha_2\theta_1}{\theta_1^2\theta_2^2}\right)d^3+\frac{2\gamma_1\alpha_2\theta_1^2-\beta_1\beta_2\theta_1\theta_2}{\theta_2^2\theta_1^2}d^2+\frac{\gamma_1\beta_2}{\theta_2}d.
\end{equation*}

In order to study the behaviour of the maxima of this function we first differentiate it and then equate it to zero. Specifically
\begin{equation*}
\frac{\dr}{\dr d}\bar{U}(d)=d^3+\frac{2}{3}\frac{\alpha_2b_1+b_2\alpha_1}{a}d^2+\frac{b_2b_1-2\alpha_2c}{2a}d-\frac{b_2c}{4a},
\end{equation*}
where $a=\alpha_1\alpha_2$, $b_1=\beta_1\lambda_1$, $b_2=\beta_2\lambda_2$ and $c=\lambda_1^2\gamma_1$. By letting $e=\frac{2}{3}\frac{\alpha_2b_1+b_2\alpha_1}{a}$, $f=\frac{b_2b_1-2\alpha_2c}{2a}$, $g=-\frac{b_2c}{4a}$ and $z=d+\frac{e}{3}$, the equation above can be set to zero as
\begin{equation*}
z^3+\left(f-\frac{1}{3}e^2\right)z +\left(g+2\left(\frac{e}{3}\right)^3-\frac{ef}{3}\right)=0
\end{equation*}
The local maxima of the conditional expected utility can therefore be described, as in \citet{Smith2012}, by the canonical cusp catastrophe where the splitting factor of this cusp catastrophe is $-(f-(1/3)e^2)$. Therefore when 
\begin{equation*}
f-\frac{1}{3}e^2\geq 0 \;\;\;\;\; \Longleftrightarrow \;\;\;\;\; \alpha_1\left(\frac{19}{54}b_2b_1\alpha_2-b_2^2\alpha_1\right)\geq \alpha_2^2\left(c\alpha_1+\frac{4}{27}b_1^2\right),
\end{equation*}
the conditional expected utility has one local maximum only: a compromise between the two attributes. 

On the other hand if $f-\frac{1}{3}e^2< 0$, then for values of $g+2\left(\frac{e}{3}\right)^3-\frac{ef}{3}$ close to zero, the expected utility function will have two local maxima, either at $d=0$ or $d=D$. In such a situations small changes of the parameters might lead to opposite optimal decision rules and agreement within the collective might be difficult to reach. 

\subsection{Bayes Linear}
We consider now Bayes linear models defined through \gls{DAG} models. These can be thought of as partial belief models since Bayes linear entails only a partial specification of the distribution of a random vector as discussed in Section \ref{sec:bayeslinear}. 

A first result we have within this framework is the following.

\begin{proposition}
\label{prop:linear}
Suppose the structural consensus of an \gls{IDSS} includes a Bayes linear \gls{DAG} model, with graph $\Gr$ having vertex set $\{Y_1,\dots,Y_n\}$. Let $B_1,\dots,B_m$ be a partition of $[n]$. If, for $i\in[n]$, panel $G_i$ delivers $\E^l_{\bm{Y}_{\Pi_{B_i}}}(\bm{Y}_{{B}_i})$ and $\V^l_{\bm{Y}_{\Pi_{B_i}}}(\bm{Y}_{B_i})$, where $\Pi_{B_i}=\cup_{j\in B_i}Pi_i$, then the collective delivers a sound Bayes linear specification of the model. 
\end{proposition} 
\begin{proof}
This is easily follows noting that the deliver beliefs are sufficient to compute the first two moments of the vector $\bm{Y}$.
\end{proof} 

It is interesting however to deduce the cases where an \gls{IDSS} embedding Bayes linear models is also adequate. By definition, this will be so only in the cases the utility function is a function of the first two moments.

\begin{proposition}
Under the conditions of Proposition \ref{prop:linear} and letting $\bm{R}=\bm{Y}$, the \gls{IDSS} is adequate iff the utility consensus includes either the class of additive utility functions where each utility function is polynomial of degree less than three, or the utility class
\begin{equation*}
u(\bm{y})=\sum_{i\in[n]}k_iu_i(y_i)+\sum_{i\in[n]}\sum_{k\in[n]_i}k_{ij}u_i(y_i)+u_j(y_j),
\end{equation*}
where each marginal utility is linear. 
\end{proposition}
\begin{proof}
This easily follows from Proposition \ref{prop:1}.
\end{proof}
Therefore the cases where Bayes linear models can be used for coherent decision making in \glspl{IDSS} are very limited. We discuss in Chapter \ref{chapter6} possible extensions of this methodology for use in \glspl{IDSS}.

\section{A Note on Some New Symbolic Models}
\label{sec:short}
This section focuses on developments of symbolic methods for inference and decision making in both \glspl{MID} and staged trees. Although these results are not discussed within the \gls{IDSS} framework, we saw in Section \ref{sec:idssex} that both these model classes can be included in the structural consensus of a coherent and distributed \gls{IDSS} under certain conditions. Therefore the results below can enhance the support \glspl{IDSS} provide in practice. 
 
\subsection{Symbolic Evaluation of Influence Diagrams}
\label{sec:symbolic}
In this section we introduce a symbolic definition of \glspl{MID} and develop a symbolic evaluation algorithm. The implementation of this algorithm with a Maple\textsuperscript{\textit{\tiny{TH}}} code is reported in Appendix \ref{appendixD}. 

Let $\Gr$ be an \gls{MID} in extensive form with vertex set $\{Y_i, u_j:j\in[m], i\in[n]\}$ and each variable $Y_i$, $i\in[n]$, takes values in $\mathcal{Y}_i=[r_{i-1}]^0$.

\subsubsection{Polynomial structure of expected utilities.}
Generalizing the work in \citet{Darwiche2003} and \citet{Castillo1995}, we introduce a symbolic representation of both the probabilities and the utilities of an  \gls{MID}. For $i\in \mathbb{V}$, $j\in[m]$, and any $y\in\mathcal{Y}_i$, $\pi\in\bm{\mathcal{Y}}_{\Pi_i}$ and  $\sigma\in \bm{\mathcal{Y}}_{P_j}$, we define the parameters\footnote{Here in order to be consistent with \citet{Leonelli2015a} and the computer code in Appendix \ref{appendixD} we use a different notation and we let $p$ and $\theta$ denote respectively probabilities and utilities.} 
\[
\begin{array}{cc}
p_{iy\pi}=\mathbb{P}(Y_i =y | \bm{Y}_{\Pi_i}=\pi), & \theta_{j\sigma}=u_j(\sigma).
\end{array}
\]
The first index of $p_{iy\pi}$ and $\theta_{j\sigma}$ refers to the random variable and utility vertex respectively. The second index of $p_{iy\pi}$ relates to the state of the variable, whilst the third one to the parents' instantiation. The second index of $\theta_{j\sigma}$ corresponds to the instantiation of the arguments of the utility function $u_j$. We take the indices within $\pi$ and $\sigma$ to be ordered from left to right in decreasing order, so that e.g. $p_{6101}$ for the \gls{MID} in Figure~\ref{fig-ex} corresponds to $\mathbb{P}(Y_6=1\;|\; Y_5=0, Y_4=1)$. 
The \textit{probability} and \textit{utility vectors}  are defined as 
 $\bm{p}_i^\T=(p_{iy\pi})_{y\in\mathcal{Y}_i, \pi\in\bm{\mathcal{Y}}_{\Pi_i}}$ and $\bm{\theta}_j^\T=(\theta_{j\pi})_{\pi\in\bm{\mathcal{Y}}_{P_j}}$, respectively, $i\in[n]$, $j\in[m]$. Parameters are listed within $\bm{p}_i$ and $\bm{\theta}_j$ according to a  lexicographic order over their indices.

\begin{example}
The symbolic parametrization of the \gls{MID} in Figure~\ref{fig-ex} is summarized in Table~\ref{para}. This is completed by the definition of the criterion weights $k_i$ and $h$ as in equations (\ref{eq:multiplicative})-(\ref{eq:h}). In Appendix \ref{appendixD} we report the symbolic definition of this \gls{MID} using our Maple\textsuperscript{\textit{\tiny{TH}}} code.
\end{example}

Because probabilities sum to one, for each $i$ and  $\pi$ one of the parameters $p_{iy\pi}$ can be written as one minus the sum of the others. 
Another constraint is induced by equation~(\ref{eq:h}) on the criterion weights. However in the following, unless otherwise indicated, we suppose that all the parameters are unconstrained.  Any unmodelled constraint can be added subsequently when investigating the geometric features of the optimal decision.

Recall that $\bar{u}_i(\bm{y}_{B_i})$, $i\in[n]$, introduced in Proposition \ref{i-th}, is the expected utility after either the marginalization or the maximization of $Y_i$. 
\begin{definition}
We define the \textbf{conditional expected utility vector} $\bar{\bm{u}}_i$ as
\begin{equation}
\label{veccond}
\bar{\bm{u}}_i^\T=(\bar{u}_i(\bm{y}_{B_i}))_{\bm{y}_{B_i}\in\bm{\mathcal{Y}}_{B_i}}.
\end{equation}
\end{definition}

In the above parametrization, $\bar{\bm{u}}_i$ consists of a vector of polynomials with unknown quantities $p_{iy\pi}$, $\theta_{j\sigma}$, $k_i$ and $h$, for $i\in[n]$, $j\in[m]$, $y\in\mathcal{Y}_i$, $\pi\in\bm{\mathcal{Y}}_{\Pi_i}$ and $\sigma\in\bm{\mathcal{Y}}_{P_j}$,  whose characteristics are specified  in  Theorem~\ref{polyexp}.
\begin{theorem}
\label{polyexp}
For an \gls{MID} $\Gr$ and $i\in [n]$, let $
c_i=\prod_{j\in B_i}r_j
$,  $u_l$ be the first utility node following $Y_i$ in the \gls{DS} of $\Gr$ and, for $j\in[m]_{l-1}$, let $w_{ij}$ be the number of random nodes % $Y_k$, $k\in \mathbb{V}$, 
between $Y_i$ and $u_j$ (including $Y_i$)  in the \gls{DS} of $\Gr$. Then $\bar{\bm{u}}_i$ is a vector of dimension $c_i$ whose entries are  polynomials including, for $a\in[m]_{l-1}$ and $b\in[a]_{l-1}$, $r_{iba}$  monomials $m_{iba}$ of degree $d_{iba}$, where
\begin{equation}
\label{struct}
\begin{array}{ccc}
r_{iba}= \binom{a-l}{b-l}\prod_{j\in[j_a]}r_j, &
d_{iba}= (b-l)+2(b-l+1)+w_{ia}, & m_{iba}=h^{b-l}m_{iba}',
\end{array}
\end{equation}
with $m_{iba}'$ a square free monomial of degree $2(b-l+1)+w_{ia}$.
\end{theorem}

 \begin{table}
\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{|l|}
\hline
$\bm{p}_2=(p_{211},p_{201},p_{210},p_{200})^{\T}$\\
$\bm{p}_3=(p_{3111},p_{3011},p_{3101},p_{3001},p_{3110},p_{3010},p_{3100},p_{3000})^{\T}$\\
$\bm{p}_5=(p_{5111},p_{5011},p_{5101},p_{5001},p_{5110},p_{5010},p_{5100},p_{5000})^{\T}$\\
$\bm{p}_6=(p_{6111},p_{6011},p_{6101},p_{6001},p_{6110},p_{6010},p_{6100},p_{6000})^{\T}$\\
$\bm{\theta}_1=(\theta_{11},\theta_{10})^{\T}$, $\bm{\theta}_2=(\theta_{21},\theta_{20})^{\T}$, $\bm{\theta}_3=(\theta_{311},\theta_{301}, \theta_{310},\theta_{300})^{\T}$\\
\hline
\end{tabular}
\end{center}
\caption{Parameterization associated to the diagram in Figure~\ref{fig-ex}. \label{para}}
\end{table}

The proof of Theorem~\ref{polyexp} is given in Appendix \ref{proof:polyexp}. Recall that, for an $a\in[m]$, $j_a$ is the index of the variable preceding $u_a$ in the \gls{DS}. We say that equation (\ref{struct}) defines the \textit{structure} of the polynomials of the conditional expected utility $\bar{\bm{u}}_i$. Specifically, the structure of a polynomial consists of its number of monomials and the number of monomials having a certain degree. In \citet{Leonelli2015a} we demonstrate that the entries of the conditional expected utility vectors of several different \glspl{MID} share the same polynomial structure. Two \glspl{MID} whose expected utility polynomials have the same structure are said to be \textit{equivalent}. By defining a lattice structure over the set of equivalent \glspl{MID}, we discuss in \citet{Leonelli2015a} how this notion of equivalence can be used for instance for model choice and sensitivity.

Since additive utility factorizations can be seen as special cases of multiplicative ones by setting $h=0$, it then follows that the conditional expected utility polynomials of \glspl{AID} are square free.

\begin{corollary} \label{corAID}
In the notation of Theorem~\ref{polyexp}, the conditional expected utility $\bar{\bm{u}}_i$, $i\in [n]$, of an \gls{AID} $\Gr$ is a vector of dimension $c_i$ whose entries are square free polynomials of degree $w_{im}+2$ including, for $a\in[m]_l$, $r_{ia}$ monomials of degree $w_{ia}+2$, where
$r_{ia}=\prod_{j\in[j_a]}r_j$.
\end{corollary}

\begin{proof}
 It follows directly from Theorem~\ref{polyexp}, since an additive factorization can be derived by setting $n_I-1$, the exponent of $h$ in equation~(\ref{eq:h}), equal to zero. This corresponds to fix $b=l$ in Theorem \ref{polyexp}. % under the notation of the theorem.
  \end{proof}% (see for more details the proof in ~\ref{appendix1}).

\begin{example}
For the \gls{MID} of Figure~\ref{fig-ex} the polynomial structure of the entries of $\bar{\bm{u}}_5$ can be constructed as follows. Recalling that all the variables are binary,  from  $B_5=\{3,4\}$ it follows that $c_5=4$. Thus, $\bar{\bm{u}}_5$ is a column vector of dimension $4$. From $u_2\equiv u_l$, it follows that
\begin{equation*}
\begin{array}{cccccc}
r_{522}=2, & r_{523}=4, &r_{533}=4,&
d_{522}=3, &d_{523}=4, &d_{533}=7,
\end{array}
\end{equation*}
using the fact that $w_{52}=1$ and $w_{53}=2$. All monomials are square free because the index $b$ of $r_{iba}$ in Theorem~\ref{polyexp} is  equal to either $l$ or $l+1$. Each entry of $\bar{\bm{u}}_5$ is a square free polynomial of degree seven consisting of ten monomials: two  of degree $3$, four of degree $4$ and four of degree $7$. 
The entry $\bar{u}_5(y_3,y_4)$ with $y_3,y_4\in[1]^0$, of this conditional expected utility  can be written as $\bar{u}_5(y_3,y_4)=\bar{u}_5^l(y_3,y_4)+\bar{u}_5^{m}(y_3,y_4)$ where
\begin{align}
&\bar{u}_5^l=k_2(\theta_{21}p_{51y_4y_3}+\theta_{20}p_{50y_4y_3})+\sum_{y_5\in[1]^0}k_3(\theta_{31y_4}p_{61y_5y_4}+\theta_{30y_4}p_{60y_5y_4})p_{5y_5y_4y_3},\label{u51}\\
&\label{u52}\bar{u}_5^{m}=hk_2k_3((\theta_{31y_4}p_{610y_4}{+\theta_{30y_4}p_{600y_4}})\theta_{20}p_{50y_4y_3}+(\theta_{31y_4}p_{611y_4}{+\theta_{30y_4}p_{601y_4})\theta_{21}p_{51y_4y_3}}).
\end{align}
 \end{example}
 
An algorithm for computing the polynomials in Theorem~\ref{polyexp} is presented in Section~\ref{alg}.

So far we have assumed that a decision centre has not provided any numerical specification of the uncertainties and the values  involved in the decision problem. 
This occurs for example if the system is defined through sample distributions of data from different experiments, where 
probabilities are known with uncertainty and the decision centre cannot provide a unique true probability. 
But in practice sometimes  
the members of the centre are able to elicit the numerical values of some parameters, which can then be simply substituted to the corresponding probability and utility parameters in the system of polynomials constructed in Theorem~\ref{polyexp} employing e.g. a computer algebra system. In such a case the degree of the polynomials and possibly their number of monomials can decrease dramatically. We present in Section~\ref{op} a plausible numerical specification of  the probabilities associated with the \gls{MID} in Figure~\ref{fig-ex}.



\subsubsection{A New Algebra For MIDs.}
\label{op}
% Now that the structure  of the polynomials associated to any \gls{MID}'s conditional expected utilities has been specified, we are ready to address the issue of computing these polynomials, 
Computing the polynomials in Theorem~\ref{polyexp} is a well known  NP-hard problem~\citep{Cooper1990}. 
Here we develop an algorithm based on three operations which exploit the polynomial structure of expected utilities and use only linear algebra calculus.  The Maple\textsuperscript{\textit{\tiny{TH}}} code for their implementation is  in Appendix \ref{appendixD}. Differently to the symbolic algorithms mentioned in Section \ref{sec:history}, our algorithm sequentially computes only monomials that  are part of the \gls{MID}'s expected utilities.
 
The operations we introduce below entail a change of dimension of the probability, utility and conditional expected utility vectors via duplication of some of their subvectors. These duplications are called \texttt{EUDuplicationTheta} and \texttt{EUDuplicationP}. We do not report here the workings of these procedure, since these are not fundamental for the following, but we report their Maple\textsuperscript{\textit{\tiny{TH}}} implementations in Appendix \ref{appendixD}. More details about these can also be found in \citet{Leonelli2015a}.  

The first of the three operations we introduce is \texttt{EUMultiSum}, which computes a weighted multilinear sum between a utility vector and a conditional expected utility.
%, where the terms of the sum are multiplied by some specific criterion weights.  
For \glspl{AID}, \texttt{EUMultiSum} consists of only summations since $h=0$.
In the algorithm in Section~\ref{alg}, an \texttt{EUMultiSum} operation is associated to every utility vertex of the \gls{MID}. 

\begin{definition}[EUMultiSum]
\label{EUMultiSum}
For $i\in [n]$, let $\bar{\bm{u}}_{i+1}$ be a conditional expected utility vector and $\bm{\theta}_j$ the utility vector of node $u_j$, $j\in [m]$, succeeding $Y_i$ in the \gls{DS}. The EUMultiSum, $+^{EU}$, between $\bar{\bm{u}}_{i+1}$ and $\bm{\theta}_j$ is defined as
\begin{enumerate}
\item $\bar{\bm{u}}_{i+1}',\bm{\theta}_j'\longleftarrow$EUDuplicationTheta($\cdot$);
\item$h\cdot k_j\cdot(\bar{\bm{u}}_{i+1}'\;\circ\;\bm{\theta}_j')\;+\;k_j\cdot\bm{\theta}_j'\;+\;\bar{\bm{u}}_{i+1}'$, where $\circ$ and  $\cdot$ denote respectively the Schur (or element by element) and the scalar products.
\end{enumerate}
\end{definition}

\begin{comment}
\begin{figure*}
\begin{center}
\begin{spacing}{1.15}
\begin{pseudocode}[ruled]{EUDuplicationTheta}{\bar{\bm{U}}_{i+1}, \bm{\theta}_j, A(i+1), P(j),\bm{r}, c_{i+1}, b_j}
\FOR k \GETS i \DOWNTO 1  \DO \BEGIN 
\IF k\in \{\{A(i+1)\cup P(j)\}\setminus \{A(i+1)\cap P(j)\}\}\THEN \BEGIN
s_k=\prod_{l=k+1}^j \mathbbm{1}_{\{l\in \{A(i+1)\cup  P(j)\}\}}(r_l) \\
\IF k\in A(i+1) \THEN \BEGIN
\bm{\theta}_j=\left(\begin{array}{ccc}
\underbrace{\begin{array}{ccc}
\bm{\theta}_j^{s_k,1}&\cdots &\bm{\theta}_j^{s_k,1}
\end{array}}_{\text{$r_k$ times}}
&\cdots&\underbrace{\begin{array}{ccc}
\bm{\theta}_j^{s_k,c_j/s_k}&\cdots &\bm{\theta}_j^{s_k,c_j/s_k}
\end{array}}_{\text{$r_k$ times}}
\end{array}\right) 
\END
\ELSEIF  k\in P(j)  \THEN \BEGIN
\bar{\bm{U}}_{i+1}=\left(\begin{array}{ccc}
\underbrace{\begin{array}{ccc}
\bar{\bm{U}}_{i+1}^{s_k,1}&\cdots &\bar{\bm{U}}_{i+1}^{s_k,1}
\end{array}}_{\text{$r_k$ times}}
&\cdots&\underbrace{\begin{array}{ccc}
\bar{\bm{U}}_{i+1}^{s_k,c_i/s_k}&\cdots &\bar{\bm{U}}_{i+1}^{s_k,c_i/s_k}
\end{array}}_{\text{$r_k$ times}}
\end{array}\right)
\END 
\END
\END \\
\RETURN{ \bar{\bm{U}}_{i+1}, \bm{\theta}_j} 
\label{dupli}
\end{pseudocode}
\end{spacing}
\end{center}
\vspace{-1.25cm}
\end{figure*}
 \end{comment}
The second operation, \texttt{EUMarginalization}, is applied to any random vertex of the \gls{MID}.
\begin{definition}[EUMarginalization]
\label{EU-Marginalization}
For $i\in\mathbb{V}$, let $\bar{\bm{u}}_{i+1}$ be a conditional expected utility vector and $\bm{p}_i$ a probability vector. The EUMarginalization, $\Sigma^{EU}$, between $\bar{\bm{u}}_{i+1}$ and $\bm{p}_i$ is defined as
\begin{enumerate}
\item $\bar{\bm{u}}_{i+1}',\bm{p}_i'\longleftarrow$EUDuplicationP($\cdot$);
\item $I_i'\times (\bar{\bm{U}}_{i+1}'\circ\bm{p}_i')$, where $\times$ denotes the standard matrix product and $I_i'$ is a matrix with $c_{i+1}s_i/r_i\in\mathbb{Z}_{\geq 1}$\footnote{This number is integer since $c_{i+1}=r_ia_{i+1}$, for an $a_{i+1}\in\mathbb{Z}_{\geq 1}$.} rows and $c_{i+1}s_i$ columns defined as 
\begin{comment}
\[
I_i'=
\left(
\begin{array}{cccc}
\bm{1}&\bm{0}&\cdots&\bm{0}\\
\bm{0}&\bm{1}&\cdots&\bm{0}\\
\vdots&&\ddots&\\
\bm{0}&\bm{0}&\cdots&\bm{1}
\end{array}
\right)
\]
\end{comment}
\[
I_i'=\left(\begin{array}{cccc}
\left(
\begin{array}{cccc}
\bm{1}&\bm{0}&\cdots&\bm{0}
\end{array}
\right)
&\left(
\begin{array}{cccc}
\bm{0}&\bm{1}&\cdots&\bm{0}
\end{array}
\right)&
\cdots&
\left(
\begin{array}{cccc}
\bm{0}&\bm{0}&\cdots&\bm{1}
\end{array}
\right)
\end{array}
\right)^\T
\]
where $\bm{1}$ and $\bm{0}$ denote row vectors of dimension $r_i$ with all entries equal to one and zero respectively and $s_i=\prod_{k\in \{\Pi_i\setminus B_{i+1}\}}r_k$.
\end{enumerate}
\end{definition}

The last operation is a maximization over the space $\mathcal{Y}_i$, $i\in \mathbb{D}$, for any element of $\bm{\mathcal{Y}}_{\Pi_i}$.

\begin{definition}[EUMaximization]
\label{EU-Maximization}
For $i\in \mathbb{D}$, let $\bar{\bm{u}}_{i+1}$ be a conditional expected utility vector. An EUMaximization over $\mathcal{Y}_i$, $\max^{EU}_{\mathcal{Y}_i}$,  is defined by the following steps: 
 \begin{enumerate}
\item set $y_i^*(\pi)=\argmax_{\mathcal{Y}_i}\bar{\bm{U}}_{i+1}$, for $\pi\in\bm{\mathcal{Y}}_{\Pi(i)}$;
\item $I_i^*\times \bar{\bm{U}}_{i+1}$, where  $I_i^*$ is a matrix with $c_{i+1}/r_{i}\in\mathbb{Z}_{\geq 1}$ rows and $c_{i+1}$ columns  defined as
\begin{comment}
\[
I_i^*=
\left(
\begin{array}{cccc}
\bm{e}_{\bm{y}_i^*(1)}&\bm{0}&\cdots&\bm{0}\\
\bm{0}&\bm{e}_{\bm{y}^*_i(2)}&\cdots&\bm{0}\\
\vdots&&\ddots&\\
\bm{0}&\bm{0}&\cdots&\bm{e}_{\bm{y}^*_i(c_{i+1}/r_{i})}
\end{array}
\right)
\]
\end{comment}
\[
I_i^*=\left(
\left(
\begin{array}{cccc}
\bm{e}_{\bm{y}_i^*(1)}&\bm{0}&\cdots&\bm{0}
\end{array}
\right)
\left(
\begin{array}{cccc}
\bm{0}&\bm{e}_{\bm{y}^*_i(2)}&\cdots&\bm{0}
\end{array}
\right)
\cdots
\left(
\begin{array}{ccc}
\bm{0}&\cdots&\bm{e}_{\bm{y}^*_i(c_{i+1}/r_{i})}
\end{array}
\right)
\right)^\T
\]
where $\bm{e}_{\bm{y}^*_i(\pi)}$, $\pi\in[c_{i+1}/r_{i}]$, is a row vector of dimension $r_{i}$ whose entries are all zero but the one in position $y_i^*(\pi)$, which is equal to one.
\end{enumerate} 
\end{definition}
The first item in Definition~\ref{EU-Maximization} is the critical part of the operation \texttt{EUMaximization}. It is not the scope of this thesis to present a methodology to identify such expected utility maximizing decisions. We simply assume that these can be found. However we note that, within our symbolic approach, polynomial optimization and semi-algebraic methods can be used to determine such optimal decisions~\citep{parrilo2000}.  Once these are identified, the EUMaximization drops the polynomials associated to non optimal courses of action. Alternatively, we can think of  the evaluation of the \gls{MID} as the computation of the expected utility polynomial of a specific policy. In this case, EUMaximization deletes the decisions that are not included in the policy. The Maple\textsuperscript{\tiny{\textit{TH}}} function \texttt{EUMaximization} in Appendix \ref{appendixD} calls a subfunction \texttt{Maximize}, which randomly picks optimal decisions. This latter function can be straightforwardly modified in such a way that, for example, it picks a decision given as input by the user. 

\begin{table}
\begin{center}
\begin{tabular}{|lllll|}
\hline
$k_1=0.2$,& $h=2.6$,& $\theta_{311} = 0$,& $p_{5101}= 0.9$&\\
$k_2=0.3$,& $\theta_{20} = 1$,& $\theta_{300} = 1$,&$p_{5110}= 0.2$,& $p_{6100}= 0.3$\\ 
$k_3=0.5$,& $\theta_{21}=0$,& $\theta_{310} = 0.4$,& $p_{5100} = 0.6$&\\
\hline
\end{tabular}
\end{center}
\caption{Numeric specification of a subset of the unknown variables associated to the diagram of Figure~\ref{fig-ex}.\label{table:num}}
\end{table}

\begin{figure}
\begin{center}
\subcaptionbox{ Optimal regions in the case $Y_3=1$.\label{fig:opt1}}
[.48\linewidth]{
\includegraphics[scale=0.8]{c}}
\subcaptionbox{Optimal regions in the case $Y_3=0$.\label{fig:opt2}}
[.48\linewidth]{
\includegraphics[scale=0.8]{c1}}
\end{center}
\caption{Regions determining the combinations of parameters leading to an optimal decision of evacuating (black regions) and not evacuating (white regions) for the evaluation of the diagram in Figure~\ref{fig-ex} given the partial numeric specification in Table~\ref{table:num}. \label{fig:opt}}
\end{figure}

\begin{example}
To illustrate the working of \texttt{EUMaximization} assume for the  \gls{MID} in Figure~\ref{fig-ex} that a decision centre has provided the specifications summarized in Table~\ref{table:num} together with the \textit{qualitative} beliefs $p_{5111} = p_{6011}$ and $p_{6010}=p_{6001}$, which in general cannot be implemented in a non-symbolic approach to decision making problems.
By plugging these numeric values and constraints into equations~(\ref{u51}) and~(\ref{u52}), the centre would choose to evacuate ($Y_4=1$) for combinations of the parameters leading to the grey areas of Figure~\ref{fig:opt}, assuming $Y_3=1$ in Figure~\ref{fig:opt1} and $Y_3=0$ in Figure~\ref{fig:opt2}. The geometric structure of the regions often gives insights about the maximization process. 
Assuming $Y_3=0$ and $\theta_{301}<1/2$, if the members of the centre believe the quantitative and qualitative statements delivered, Figure~\ref{fig:opt2} shows that evacuation will be the optimal choice, no matter what values are given to $p_{6010}$ and $p_{5111}$. 

In other cases the situation might not be as clear and no optimal decisions can be identified. We can however for example envisage the algorithm to work over the two sub-regions of Figure~\ref{fig:opt} separately. The algorithm would then output different optimal courses of action for different combinations of the unknown parameters. The development of such methodologies is subject of ongoing research. 
\end{example}

Each of the above three operations changes the conditional expected utility vectors and their entries in a specific way we formalize below.

\begin{proposition}
\label{polyop}
For $i\in [n]$, let $\bar{\bm{u}}_{i+1}$ be a conditional expected utility vector whose entries have the polynomial structure of Equation~(\ref{struct}) and let $U_j$ be the vertex preceding $Y_{i+1}$ in the \gls{DS}. Then in the notation of Theorem~\ref{polyexp}
\begin{itemize}
\item $\max^{EU}_{\mathcal{Y}_i}\bar{\bm{U}}_{i+1}$ has dimension $c_{i+1}/r_i$ and its entries  do not change polynomial structure;
\item $\bar{\bm{u}}_{i+1} +^{EU} \bm{\theta}_{j}$ has dimension $c_{i+1}s_{i}^\mathbb{U}$, where $s_i^\mathbb{U}=\prod_{k\in \{P_j\setminus B_{i+1}\}}r_k$, and each of its entries includes $r_{(i+1)ba}$ monomials of degree $d_{(i+1)ba}$, $r_{(i+1)ba}$ monomials of degree $d_{(i+1)ba}+3$ and one monomial of degree 2;
\item $\bar{\bm{u}}_{i+1}\Sigma^{EU}\bm{p}_i$ has dimension $c_{i+1}s_i/r_i$, where $s_i=\prod_{k\in \{\Pi_i\setminus B_{i+1}\}}r_k$, and each of its entries includes $r_ir_{(i+1)ba}$ monomials of degree $d_{(i+1)ba}+1$.
\end{itemize} 
\end{proposition}
This result directly follows from the definition of the above three operations whose effect on the polynomials associated to the diagram in Figure~\ref{fig-ex} is illustrated below.

\subsubsection{A Fast Evaluation Algorithm.} \label{alg}
The algorithm for the evaluation of \glspl{MID} is given in Algorithm~\ref{algo}. This receives in input the \gls{DS} of the \gls{MID}, $S$ say, the sets $\mathbb{J}$, $\mathbb{V}$ and $\mathbb{D}$, and the vectors $\bm{p}=(\bm{p}_i,\dots,\bm{p}_n)_{i\in[n]}^\T$, $\bm{\theta}=(\bm{\theta}_j)_{j\in[n]}^\T$ and $\bm{k}=(h,k_j)_{j\in[m]}^\T$.\footnote{Note that the inputs of this function in Maple\textsuperscript{\tiny{\textit{TH}}} are different in order to better exploit the potentiality of the computer algebra software.} 
This corresponds to a symbolic version of the backward induction procedure over the elements of the \gls{DS} explicated in Proposition \ref{i-th}. At each the inductive step, a utility vertex is considered together with the variable that precedes it in the \gls{DS}.

In line (1) the conditional expected utility $\bar{\bm{u}}_{n+1}$  is initialized to $(0)$.   
Lines (2) and (3) index a reverse loop over the indices of both the variables and the utility vertices respectively (starting from $n$ and $m$). If the current index corresponds to a variable preceding a utility vertex in the \gls{DS} (line 4), then the algorithm jumps to lines (5)-(7). Otherwise it jumps to lines (8)-(10). In the former case, the algorithm computes, depending on whether or not the variable is controlled (line 5), either an \texttt{EUMaximization} over $\mathcal{Y}_k$ (line 6) or an \texttt{EUMarginalization} (line 7) with $\bm{p}_k$,  jointly to an \texttt{EUMultiSum} with $\bm{\theta}_l$. 
In the other case, \texttt{EUMaximization} and \texttt{EUMarginalization} operations are performed without \texttt{EUMultiSum}. The Maple\textsuperscript{\tiny{\textit{TH}}}
function \texttt{SymbolicExpectedUtility}  in Appendix \ref{appendixD} is our implementation of  Algorithm~\ref{algo}.


\begin{figure*}
\begin{center}
\begin{spacing}{1.15}
\begin{pseudocode}[ruled]{SymbolicExpectedUtility}{\mathbb{J},S,\bm{p},\bm{\theta}, \bm{k},\mathbb{V},\mathbb{D}}
\label{algo}
\bar{\bm{u}}_{n+1}=(0)\hspace{10.79cm}(1)\\
\FOR k \GETS n \DOWNTO 1 \hspace{9.04cm}(2)\DO \BEGIN 
\FOR l \GETS m \DOWNTO 1 \hspace{7.74cm}(3)\DO \BEGIN
\IF k=j_l \hspace{8.67cm} (4)\THEN \BEGIN
\IF k\in \mathbb{D} \hspace{7.02cm}(5)\THEN\BEGIN
\bar{\bm{u}}_{k}=\max^{EU}_{\mathcal{Y}_k}(\bar{\bm{u}}_{k+1}+^{EU}\bm{\theta}_l)\hspace{2.21cm}(6)
\END
\ELSE \BEGIN
\bar{\bm{u}}_k=\bm{p}_k\;\Sigma^{EU}\;(\bar{\bm{u}}_{k+1}+^{EU}\bm{\theta}_{l}) \hspace{2.19cm}(7)
\END
\END
\ELSEIF k\in \mathbb{D} \hspace{7.55cm}(8)\THEN \BEGIN
\bar{\bm{u}}_k=\max^{EU}_{\mathcal{Y}_k}\bar{\bm{u}}_{k+1}\hspace{5.41cm}(9)
\END
\ELSE \BEGIN
\bar{\bm{u}}_k=\bm{p}_k\;\Sigma^{EU}_{\mathcal{Y}_k}\;\bar{\bm{u}}_{k+1}\hspace{5.26cm}(10)
\END
\END
\END\\
\RETURN{ \bar{\bm{U}}_{1}} \hspace{10.29cm} (11)
\label{algo1}
\end{pseudocode}
\end{spacing}
\end{center}
\vspace{-1.25cm}
\end{figure*}

\begin{example}
For the \gls{MID} in Figure~\ref{fig-ex} the \texttt{SymbolicExpectedUtility} function first considers  the random vertex $Y_6$ which precedes the utility vertex $U_3$ and therefore first calls the \texttt{EUMultiSum} function. \texttt{EUDuplicationTheta} in this case entails that $\bar{u}_7$ is duplicated four times and 
\begin{equation}
\label{ms3}
\bar{\bm{U}}_7+^{EU}\bm{\theta}_3=\left(
\begin{array}{cccc}
k_3\theta_{11}&
k_3\theta_{01}&
k_3\theta_{10}&
k_{3}\theta_{00}
\end{array}
\right)^\T.
\end{equation}
Then, the right hand side of Equation~(\ref{ms3}) needs to be duplicated via \texttt{EUDuplicationP} and  
\begin{equation*}
\label{eq:ciao1}
\bar{\bm{u}}_6^\T=I'_6 \times \bar{\bm{u}}_6^t\circ \bm{p}_6= \left(k_3\theta_{31j}p_{61ij}+k_3\theta_{30j}p_{60ij}\right)_{i,j\in[1]^0},
\end{equation*}
where $\bar{\bm{u}}^t_6$ is equal to the duplicated version of the right hand side of equation~(\ref{ms3}).
The vector $\bar{\bm{u}}_6$ has dimension four and its entries include two monomials of degree $3$. Since the random vertex $Y_5$ is the unique parent of $U_2$ the \texttt{SymbolicExpectedUtility} function follows the same steps as before. \texttt{EUMultiSum} is first called which gives as output
\begin{equation*}
\label{u6p}
\bar{\bm{u}}_6+^{EU}\bm{\theta}_2= h\cdot k_2\cdot \bar{\bm{U}}_6\circ \left(
\begin{array}{cccc}
\theta_{21}&
\theta_{20}&
\theta_{21}&
\theta_{20}
\end{array}
\right)^\T+\bar{\bm{U}}_6+k_2\cdot\left(
\begin{array}{cccc}
\theta_{21}&
\theta_{20}&
\theta_{21}&
\theta_{20}
\end{array}
\right)^\T
\end{equation*}
The polynomial $\bar{\bm{u}}_6+^{EU}\bm{\theta}_2$ is the sum of two monomials of degree $3$ inherited from $\bar{\bm{u}}_6$, of two monomials of degree $6$ (from the first term on the rhs of~\ref{u6p}) and one monomial of degree $2$ (from the last term on the rhs of~\ref{u6p}). Its dimension is equal to four (that is, no \texttt{EUDuplicationTheta} is required). 
\texttt{EUMultiSum}  manipulates the conditional expected utility vector according to Proposition~\ref{polyop}. Then the \texttt{EUMarginalization} function computes
\[
\bar{\bm{u}}_5=
I_5'\times
\left(\left(\begin{array}{cc}
\bar{\bm{u}}_6+^{EU}\bm{\theta}_2&
\bar{\bm{u}}_6+^{EU}\bm{\theta}_2
\end{array}
\right)^\T\circ
\bm{p}_5\right).
\] 
Each entry of $\bar{\bm{u}}_5$ has twice the number of monomials of the entries of $\bar{\bm{u}}_6+^{EU}\bm{\theta}_2$ and each  monomial of $\bar{\bm{u}}_5$ has degree $d+1$, where $d$ is the degree of each monomial of $\bar{\bm{u}}_6+^{EU}\bm{\theta}_2$ (whose entries are homogeneous polynomials). These vectors also have the same dimension. Thus, this \texttt{EUMarginalization} changes the conditional expected utility vector according to Proposition~\ref{polyop}. The polynomial in a generic entry of $\bar{\bm{u}}_5$ was shown in equations (\ref{u51})-(\ref{u52}).

The algorithm then considers the controlled variable $Y_4$. Since $4\not\in \mathbb{J}$, $Y_4$ is not the argument of a utility function with the highest index and therefore the algorithm calls the \texttt{EUMaximization} function. Suppose the optimal decisions are identified to be $Y_4=1$ when $Y_3=1$ and $Y_4=0$ when $Y_3=0$. The evaluation would then suggest that the population is evacuated whenever a high level of deposition is observed and that people are not evacuated if the deposition is low. Then \texttt{EUMaximization}  would  return $\bar{\bm{u}}_4=I^{*}_4\times \bar{\bm{u}}_5$, where $I^{*}_4$ is a $2\times 4$ matrix with ones in positions $(1,1)$ and $(2,4)$ and zeros otherwise.
Proposition~\ref{polyop} is respected since the entries of $\bar{\bm{u}}_4$ have the same polynomial structure of those of $\bar{\bm{u}}_5$ and $\bar{\bm{u}}_4$ has dimension $2$.

The \texttt{SymbolicExpectedUtility} function then applies in sequence the operations defined in  Section~\ref{op}. For the \gls{MID} in Figure~\ref{fig-ex} it sequentially computes the following quantities:
\[
\begin{array}{ll}
\bar{\bm{u}}_3^t=h\cdot k_1\cdot \bar{\bm{u}}_4\circ\bm{\theta}_1
+\bar{\bm{u}}_4+ k_1\cdot 
\bm{\theta}_1,
& \bar{\bm{u}}_3 = I_3^{'}\times \left(\left(
\begin{array}{cccc}
\bar{\bm{u}}_3^t&
\bar{\bm{u}}_3^t&
\bar{\bm{u}}_3^t&
\bar{\bm{u}}_3^t
\end{array}
\right)^\T
\circ\bm{p}_3\right),\\
\bar{\bm{u}}_2=I_2^{'}\times
\left(\bar{\bm{u}}_3\circ
\bm{p}_2
\right),
&\bar{\bm{u}}_1=\left(
\begin{array}{cc}
1&0
\end{array}
\right)\times 
\bar{\bm{u}}_2,
\end{array}
\]
assuming the optimal initial decision is $Y_1=1$.

Interestingly, using the new algebra we introduced in Section~\ref{op}, the evaluation of an \gls{MID} can be written as a simple algebraic expression. For example, the evaluation of the \gls{MID} in Figure~\ref{fig-ex} can be written as
\begin{equation*}
\bar{\bm{U}}_1=\max\nolimits^{EU}_{\mathcal{Y}_1}\Bigg(\bm{p}_2 \;\Sigma^{EU}\;\bigg(\bm{p}_3\;\Sigma^{EU}\;\Big(\bm{\theta}_1+^{EU}\max\nolimits^{EU}_{\mathcal{Y}_4}\big(\bm{p}_5\;\Sigma^{EU}(\bm{\theta}_2+^{EU}(\bm{p}_6\;\Sigma^{EU}\bm{\theta}_3))\big)\Big)\bigg)\Bigg)
\end{equation*}
\end{example}
and this polynomial can be evaluated  with \texttt{SymbolicExpectedUtility}.

Although we have developed in this section a symbolic approach for the evaluation of extensive form \glspl{MID}, in \citet{Leonelli2015a} we formalize both the symbolic and the polynomial interpretation of the manipulations (edge reversal and barren node removal) of non extensive form diagrams. In that paper we further deduce the polynomial and symbolic interpretation of the sufficiency theorem of \citet{Smith1989}, that can be used to greatly simplify the evaluation of an \gls{MID} by deleting some of its vertices.
 
\begin{comment}
\subsubsection{Symbolic Manipulation of Influence Diagrams.}

Arc reversal and barren node removal change the symbolic parametrization of the \gls{MID} according to Proposition~\ref{par}. 
After an arc reversal, the diagram $G'$ includes the edge $(Y_j,Y_i)$ where $i<j$. Algorithm~\ref{algo}, and similarly the \texttt{SymbolicExpectedUtility} Maple\textsuperscript{\textit{\tiny{TH}}} function, works through a backward induction over the indices of the variables and, by construction, always either marginalize or maximize a vertex before its parents. It cannot therefore be applied straightforwardly to the diagram $G'$. We define here the adjusted Algorithm~\ref{algo} which takes into account the reversal of an arc by, roughly speaking, switching the order in which the  variables associated to the reversed edge are marginalized during the procedure.  Specifically, in the adjusted Algorithm~\ref{algo} a marginalization operation is performed over $Y_i$ at the $n-j+1$ backward inductive step, whilst for $Y_j$ this happens at the $n-i+1$ step. Therefore $\bar{U}_j'$ is the conditional expected utility associated to $G'$ after the marginalization of $Y_i$ and  $\bar{U}_i'$ is the conditional expected utility after the marginalization of $Y_j$. Note that under this operation the sets $\mathbb{J}$ and $A(i)$, $i\in [n]$, might change: we respectively call $\mathbb{J}'$ and $A'(i)$ the ones associated to $G'$.

\begin{proposition}
\label{par}
Under the conditions of Proposition~\ref{manipulation}, 
let $p_{iy\pi}'$ and $\Pi'(i)$  be a parameter and a parent set associated to the diagram $G'$ resulting from arc reversal and barren node removal.  

\begin{itemize}
\item For $i,j\in\mathbb{V}$, if $\Pi'(i)$ and $\Pi'(j)$ are the parent sets of $Y_i$ and $Y_j$ after the reversal of the edge $(Y_i,Y_j)$, then 
the polynomial associated to $G'$ is
\begin{equation*}
p_{iy_i\pi'(i)}'= \frac{p_{jy_j\pi(j)}p_{iy_i\pi(i)}}{\sum_{y_i=0}^{r_i-1}p_{jy_j\pi(j)}p_{iy_i\pi(i)}},
\hspace{1cm}p_{jy_j\pi'(j)}'=\sum_{y_i\in\mathcal{Y}_i}p_{jy_j\pi(j)}p_{iy_i\pi(i)},
\end{equation*}
for $\pi(i)\in\mathcal{Y}_{\Pi(i)}$, $\pi(j)\in\mathcal{Y}_{\Pi(j)}$, $\pi'(i)\in\mathcal{Y}_{\Pi'(i)}$,$\pi'(j)\in\mathcal{Y}_{\Pi'(j)}$, $y_i\in\mathcal{Y}_i$ and $y_j\in\mathcal{Y}_j$.
\item For $i,j\in \mathbb{V}$,
assume that after the reversal of the edges $(Y_i,Y_j)$, for every children $Y_j$ of $Y_i$, $Y_i$ is now a barren node and let $\Pi^i(j)=\Pi(j)\setminus \{i\}$. Then
\begin{itemize}
\item in the new parameterization $\bm{p}_i'$ is deleted.
\item in the original parameterization $\bm{p}_i$ is deleted and $p_{jy_j\pi^i(j)0}=\cdots=p_{jy_j\pi^i(j)r_i-1},$ for $y_j\in\mathcal{Y}_j$, $\pi^i(j)\in\mathcal{Y}_{\Pi^i(j)}$, where the fourth index of $p_{jy_j\pi^i(j)i}$, $i\in[r_1-1]$, refers to the instantiation of $Y_i$.
\end{itemize}
\end{itemize}
\end{proposition}
 The proof of this proposition is reported in~\ref{proofteo2}.

\begin{example}
Reversing the edge $(Y_2,Y_3)$ in the \gls{MID} of Figure~\ref{fig-ex1}, by Proposition~\ref{par} %, the  following two reparameterizations  need to be adopted 
we obtain:
\begin{equation*}
p_{3y_3y_1}'=p_{3y_31y_1}p_{21y_1}+p_{3y_30y_1}, \hspace{1cm}p_{20y_1}p_{2y_2y_3y_1}'=\frac{p_{3y_3y_2y_1}p_{2y_2y_1}}{p_{3y_31y_1}p_{21y_1}+p_{3y_30y_1}p_{20y_1}}.
\end{equation*}
for $y_1,y_2,y_3\in\{0,1\}$. Proposition~\ref{par} also specifies that the deletion of the vertex $Y_2$ in Figure~\ref{fig-ex2} simply corresponds to canceling the vectors $\bm{p}_2$ and $\bm{p}_2'$ and setting $p_{3y_31y_1}$ equal to $p_{3y_30y_1}$ for any $y_1,y_3\in\{0,1\}$.
\end{example}
 
A  consequence of Proposition~\ref{par} is that manipulations of the diagram change the polynomial structure of the conditional expected utilities under the new parametrization $\bm{p}'$. We assume here for simplicity that $i\not \in P(j)$, $j\in [m]$. There is no loss of generality in this assumptions since arguments of utility functions cannot be deleted from the diagram without changing the result of the evaluation of the \gls{MID}.

\begin{lemma}
\label{polarc}
Under the assumptions of Theorem~\ref{par}  and in the notation of Theorem~\ref{polyexp} the following holds.
\begin{enumerate}
\item Let $x$ be the smallest index in $\Pi(i)\cup \Pi(j)$, reverse of the arc $(Y_i, Y_j)$ and evaluate the \gls{MID} using the adjusted Algorithm~\ref{algo}.
	\begin{enumerate} %\todo{sono coperti tutti i casi di $j$?}
	\item If $j\not \in \mathbb{J}$, then
		\begin{enumerate}
		\item $\bar{U}_j'(\bm{y}_{A'(j)})$ has $r_ir_{jba}/r_j\in\mathbb{Z}_{\geq 1}$\footnote{This is so since $r_{jba}=r'r_j$ for some $r'\in\mathbb{Z}_{\geq 1}$.} monomials of degree $d_{jba}$;
		         for $i<k<j$, $\bar{U}_k'(\bm{y}_{A'(k)})$ can have different polynomial structure from
		         $\bar{U}_k(\bm{y}_{A(k)})$ according to Proposition~\ref{polyop}.
		\item The vectors $\bar{\bm{U}}_k'$, $x<k\leq j$, have dimension $c_k'=\prod_{s\in \{A'(k)\setminus\{k,\dots,n\}\}}r_s$  where 
		         $ A'(k)=A(k)\cup\{l: (Y_x,Y_i) \mbox{ or } (Y_x,Y_j) \in E(G')\}$.
		\end{enumerate}
	\item \label{1b} If $j\in \mathbb{J}\cap \mathbb{J}'$, then
		\begin{enumerate}
		\item $\bar{U}_j'(\bm{y}_{A'(j)})$ has $r_ir_{(j+1)ba}$ monomials of degree $d_{(j+1)ba}+1$ and, for $i<k<j$, $\bar{U}_k'(\bm{y}_{A'(k)})$ has a different polynomial structure from $\bar{U}_k(\bm{y}_{A(k)})$ that can be deduced via Proposition~\ref{polyop}.
		\item For $x<k<j$, $\bar{\bm{U}}_k'$ has dimension  $ c_k''=\prod_{s\in \{A''(k)\setminus \{k,\dots,n\}\}}r_s$, with $A''(k)=A'(k)\cup P(j_j)$.
		\end{enumerate}
	\item If $j\not\in \mathbb{J}' \cap P(t)$ and  $s$ is the second highest index in $P(t)$, then
		\begin{enumerate}
		\item for $s<k\leq j$, $\bar{U}_k'(\bm{y}_{A'(k)})$ has the polynomial structure specified in point 1.(b) and dimension $c_k''$; 
		\item $ i< k\leq s$, $\bar{U}_k'(\bm{y}_{A'(k)})$ has the polynomial structure specified in point 1.(a) and dimension $c_k'$. 
		\item For $x<k\leq i$, $\bar{\bm{U}}_k'$ has dimension $c_k'$ and the polynomial structure of its entries does not change.
		\end{enumerate}
	\end{enumerate}
\item Let $Y_z$ be the child of $Y_i$ in $G$ with the highest index and remove the barren node $Y_i$ in $G'$. Then
		\begin{enumerate}
		\item  $\bar{\bm{U}}_k'$, $i<k\leq z $, has  $c_k'/r_i$ entries whose polynomial structure does not change,
		\item for $k\leq i$, $\bar{\bm{U}}_k'$ has dimension $c_k'$ and each of its entries has $r_{kba}/r_i$ monomials of degree $d_{kba}-1$.
         	\end{enumerate}
 \end{enumerate}

\end{lemma}
The proof of this lemma is provided in~\ref{corollario}.

\begin{example}
After the reversal of the edge $(Y_2,Y_3)$ from the network in Figure~\ref{fig-ex1}, the polynomial structure of the conditional expected utilities associated to the original and to the manipulated diagrams is reported in the first six columns of Table~\ref{ciao}. 
Since $Y_3$ is the only argument of $U_1$ we are in Item~(\ref{1b}) of Lemma~\ref{polarc}.
%Note that this arc reversal corresponds to the case $j\in\mathbb{J} \textcolor{blue}{\cap} \mathbb{J}'$ since $Y_3$ is the only argument of $U_1$. 
The conditional expected utility $\bar{\bm{U}}^r_3$ obtained running the adjusted Algorithm~\ref{algo} for the network of Figure~\ref{fig-ex2} after the marginalization of $Y_2$. %, changes according to Lemma~\ref{polarc}. 
Furthermore, $\bar{\bm{U}}_2^r$ and $\bar{\bm{U}}_1^r$ have the same polynomial structures as  $\bar{\bm{U}}_2$ and $\bar{\bm{U}}_1$. 
The last $3$ columns of the Table~\ref{ciao} show the polynomial structure of the conditional expected utilities $\bar{\bm{U}}_3^b$ associated to the \gls{MID} in Figure~\ref{fig-ex3} which does not include $Y_2$. 
According to Lemma~\ref{polarc}, $\bar{\bm{U}}_3^b$ has the same polynomial structure of $\bar{\bm{U}}_3$
and for each row of the table, the number of monomials with degree $d$ in $\bar{\bm{U}}_1^b$ is half the number of monomials of $\bar{\bm{U}}_1$ having degree $d+1$. 
Similarly the dimension of these conditional expected utility vectors respects Lemma~\ref{polarc}.
\end{example}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 \multicolumn{3}{|c|}{$\bar{\bm{U}}_2\equiv \bar{\bm{U}}_1$}&\multicolumn{3}{|c|}{$\bar{\bm{U}}_3$}&\multicolumn{3}{|c|}{$\bar{\bm{U}}_3^r$} &\multicolumn{3}{|c|}{$\bar{\bm{U}}_2^r\equiv \bar{\bm{U}}_1^r$}&\multicolumn{3}{|c|}{$\bar{\bm{U}}_3^b\equiv \bar{\bm{U}}^b_1$}\\
\hline
$\#$&d.&s.f.&$\#$&d.&s.f.&$\#$&d.&s.f.&$\#$&d.&s.f.&$\#$&d.&s.f.\\
\hline
4&4&yes&2&3&yes&4&4&yes&4&4&yes&2&3&yes\\
8&5&yes&4&4&yes&8&5&yes&8&5&yes&4&4&yes\\
16&6&yes&8&5&yes&8&8&yes&16&6&yes&8&5&yes\\
8&8&yes&4&7&yes&&&&8&8&yes&4&7&yes\\
32&9&yes&16&8&yes&&&&32&9&yes&16&8&yes\\
16&12&no&8&11&no&&&&16&12&no&8&11&no\\
\hline
\end{tabular}
\end{center}
\caption{Polynomial structure of the conditional expected utilities for the original \gls{MID}, $\bar{\bm{U}}_j$, for the one after the reversal of the arc $(Y_2,Y_3)$, $\bar{U}_j^r$ and for the one after the removal of the barren node $Y_2$, $\bar{\bm{U}}_j^b$. The symbol $\#$ corresponds to the number of monomials, d. to the degree and s.f. whether or not they are square free. }\label{ciao}
\end{table}

We now formalize how the sufficiency principle changes the symbolic parametrization of the \gls{MID}.

\begin{proposition}
\label{parsuff}
Let $i,j,k\in\mathbb{V}$ and $G$ be a \gls{MID}.
Let $Y_i$ be a vertex removed after the application of the sufficiency principle to $G$ and $G'$ the obtained  \gls{MID}.
Assume $Y_i$ to be a father of $Y_k$ and 
a parent (not a father) of $Y_j$ in $G$. 
Let $\Pi'(k)$ be the parent set of a vertex $Y_k$ in $G'$ and for $l\in [n]$
\begin{align*}
\Pi^i_>(k)=\{\Pi(k)\setminus \{1,\dots, i-1\} \},\hspace{1cm}
\Pi^k_i(l)=\{\Pi(l)\cap \Pi(k) \cap \Pi(i)\}.
\end{align*}
Then the reparametrization of the  \gls{MID}  with graph  $G'$ is
\begin{align*}
p'_{ky_k\pi'(k)}=\sum_{y_i\in\mathcal{Y}_i}p_{ky_k\pi(k)}p_{iy_i\pi(i)},\hspace{1cm}
p'_{jy_j\pi'(j)}=\sum_{y_j\in\mathcal{Y}_j}p_{jy_j\pi(j)}\frac{\prod_{l\in \Pi^i_{>}(j)}\sum_{y_{\Pi^j_i(l)}\in\mathcal{Y}_{\Pi^j_i(l)}} p_{ly_l\pi(l)}p_{iy_i\pi(i)}}{\sum_{y_i=0}^{r_i-1}\prod_{l\in \Pi^i_{>}(j)}\sum_{y_{\Pi^j_i(l)}\in\mathcal{Y}_{\Pi^j_i(l)}} p_{ly_l\pi(l)}p_{iy_i\pi(i)}}
\end{align*}
\end{proposition}
The proof of this proposition is provided in~\ref{teorema}.
Again, this new parametrization $\bm{p}'$  implies a change in the conditional expected utility vectors.
\begin{lemma}
\label{polsuff}
Assume the vertex $Y_i$ is removed using the sufficiency principle and that $Y_j$ is the child of $Y_i$ with the highest index.  Under the notation of Theorem~\ref{polyexp} the conditional expected utility vectors in $G'$ are such that
\begin{enumerate}
\item \label{bullet1} for $k<i$, the entries of $\bar{\bm{U}}_k'$ have $r_{kba}/r_i$ monomials of degree $d_{kba}-1$, while for $k>i$ their structure does not change.
\item \label{bullet2} for $k\leq j$, $\bar{\bm{U}}_k$ has now dimension $\prod_{s\in A'(k)}r_s$, where $A'(k)=\{A(k)\cup \Pi(i)\setminus \{k,\dots,n\}\}$, while for $k>j$ its dimension does not change.
\end{enumerate}
\end{lemma}
\begin{proof}
Item~\ref{bullet1} of Corollary~\ref{polsuff} is a straightforward consequence of Proposition~\ref{polyop}, since the deletion of the vertex $Y_i$ entails one less EUMarginalization during Algorithm~\ref{algo}. Item~\ref{bullet2} of Corollary~\ref{polsuff} follows from the fact that the sets $A(i)$ and $A'(i)$ only affect the dimension of the conditional expected utility vectors. \end{proof}

Since the application of the sufficiency principle to the diagram of Figure~\ref{fig-ex1} provides the same output network as the one obtained from the reversal of the edge $(Y_2,Y_3)$ and the removal of $Y_2$, an illustration of these results can be found in Table~\ref{ciao}.

\subsubsection{The Lattice of Equivalent Influence Diagrams.}
\label{latt}
Theorem~\ref{polyexp}  specifies the polynomial structure of the expected utility associated to any \gls{MID} at any stage of its evaluation. However, as we show in this section, there are many \glspl{MID} whose conditional expected utilities share the same polynomial structure. Such  \glspl{MID}  are called \textbf{equivalent}.


\begin{definition}
\label{equivalent}
Two \glspl{MID} in extensive form are equivalent the following conditions hold: 
\begin{multicols}{2}
\begin{enumerate}
\item they have the same vertices;
\item they imply the same total order over the decision variables;
\item they have the same sets $P[j]$, $j\in[m]$;
\item they imply the same partial order over the vertices;%$\{Y_1,\dots,Y_n\}$;
\item they exhibit the same utility factorization.
\end{enumerate}
\end{multicols}
\end{definition}

Points~1-3 of Definition~\ref{equivalent} specify that the decision problems associated to equivalent \glspl{MID} can be modelled by \glspl{MID} with the same vertex sets, more specifically that there is a one-to-one mapping between random variables and decision variables (point 1), that the order in which decisions are committed to is equal (point 2) and that the utility functions have the same arguments (point 3). Point~4 implies a simplification which does not limit generality. 
It is imposed because different partial orders would require a completely different parameter sets. 
Note however that different partial orders describing the same conditional independence structure are associated to the same probability density factorization and therefore lead to evaluations with the same output~\citep[see e.g.][]{smith89}. Finally, point~5 guarantees that equivalent \glspl{MID} have the same utility vertices and the same edges terminating in a utility node. %$U_j$, $j\in[m]$. Recall also that we consider here only \glspl{MID} in extensive form.
In particular the edge sets of equivalent \glspl{MID} can differ only for edges % from a variable $Y_i$, $i\in[n]$, to 
into a non controlled variable. % $Y_j$, $j\in\mathbb{V}$.
Specifically, equivalent \glspl{MID} differ at most for the probabilistic structure they imply. 
 This is formalized by Proposition~\ref{equival}.


\begin{figure}
\begin{center}
\vspace{1.3cm}
\subcaptionbox{Minimal \gls{MID} equivalent to one in Figure~\ref{fig-ex}. \label{fig-min}}
[.48\linewidth]{\scalebox{0.9}{
\xymatrix{
&&&&U_2\\
*+[F]{Y_1}\ar@/^2pc/[rrr]\ar[r]&*+[Fo]{Y_2}\ar@/^2pc/[rr]\ar[r]&*+[Fo]{Y_3}\ar[r]\ar[d]&*+[F]{Y_4}\ar[r]\ar[dr]&*+[Fo]{Y_5}\ar[r]\ar[u]&*+[Fo]{Y_6}\ar[dl]\\
&&U_1&&U_3
}}}
\subcaptionbox{Maximal \gls{MID} equivalent to the on Figure~\ref{fig-ex}.\label{fig-max}}
[.48\linewidth]{\scalebox{0.9}{
\xymatrix{
&&&&U_2\\
*+[F]{Y_1}\ar@/^2pc/[rrr]\ar[r]\ar@/_2pc/[rr]\ar@/^3pc/[rrrr]\ar@/^7pc/[rrrrr]&*+[Fo]{Y_2}\ar@/^2pc/[rr]\ar@/^2pc/[rrr]\ar@/^6pc/[rrrr]\ar[r]&*+[Fo]{Y_3}\ar[d]\ar@/^2pc/[rr]\ar@/^5pc/[rrr]\ar[r]&*+[F]{Y_4}\ar@/^4pc/[rr]\ar[r]\ar[dr]&*+[Fo]{Y_5}\ar[r]\ar[u]&*+[Fo]{Y_6}\ar[dl]\\
&&U_1&&U_3
}}}
\caption{Two \glspl{MID} in the same equivalence class of the diagram in Figure~\ref{fig-ex}}\label{maxmin}
\end{center}

\end{figure}

\begin{proposition}
\label{equival}
An \gls{MID} $G$ is equivalent to an \gls{MID} $G'$ if and only if
\begin{itemize}
\item $V(G)=V(G')$;
\item if $(Y_i,U_j)\in E(G)$, for $j\in[m]$ and $i\in [n]$, then $(Y_i,U_j)\in E(G')$;
\item if $(Y_i,Y_j)\in E(G)$, for $i\in\mathbb{V}$ and $j\in \mathbb{D}$, then $(Y_i,Y_j)\in E(G')$. 
\end{itemize}
\end{proposition}

\begin{example} \rm 
The \glspl{MID} in Figures~\ref{maxmin} and~\ref{fig-ex} are equivalent according to Proposition~\ref{equival}.
\end{example}


The equivalence between \glspl{MID} of Definition~\ref{equivalent} is a proper equivalence relation \textcolor{blue}{and conditional expected utilities of \glspl{MID} in the same equivalence class share the same polynomial structure. This is stated in Proposition~\ref{polequi} whose proof is immediate.}
\textcolor{blue}{??Consider for example the set of \glspl{MID} defined by the points 1-4 of Definition~\ref{equivalent}. This set can be partitioned for example by the equivalence classes of \glspl{MID} exhibiting the same utility factorization.??}

\begin{proposition}
\label{polequi}
\glspl{MID} in an equivalence class have conditional expected utility vectors \textcolor{blue}{  with possibly different variables/parametrization/monomials??. 
cancellerei di qui alla fine della proposizione perche' mi sembra ovvio, $\bar{\bm{U}}_i$, $i\in[n]$, whose entries have the same polynomial structure. }
\end{proposition}

\begin{corollary}
The only difference between  \glspl{MID} in the same equivalence class might consist of a different dimension for some of their conditional expected utility vectors.
\end{corollary}
\begin{proof}  This result follows directly from the characterization of equivalence in Proposition~\ref{equival}, since the sets $A(i)$, $i\in [n]$, within two equivalent \glspl{MID} are the only objects that can differ and these can only affect the dimension of the conditional expected utility vectors. \end{proof}

The different dimensions  of conditional expected utility vectors of \textcolor{blue}{equivalent \glspl{MID} } are actually \textcolor{blue}{?? constrained by a rather strong order over the elements of $\mathcal{C}$.??Secondo me bisogna essere piu' chiari nella Proposizione 7} Specifically,  \textcolor{blue}{this is a lattice as shown in Section~\ref{latticeequival}.} %that this order can be associated to an underlying lattice. Before studying the characteristics of the lattice of equivalent \glspl{MID}, we need to provide a short introduction to such structures. 


% Before proceeding with the definition of the lattice over the elements of an equivalence class $\mathcal{C}$, we specifically define two of its elements.
In this section the lattice structure of an equivalence class  $\mathcal{C}$ of \glspl{MID} is studied. 
\begin{definition} 
\begin{enumerate}
\item An \gls{MID} $G_{min} \in \mathcal{C}$ is said to be \textit{minimal} if for any $i\in [n]$ and for $j\in \mathbb{V}$
\[
(Y_i,Y_j)\in E(G_{min})  \text{ if and only if } Y_j \mbox{ covers } Y_i
\]
 with respect to the partial order over $\{Y_1,\dots,Y_n\}$.
The number of edges in $E(G_{min})$ is denoted with $n_{min}$.
\item
An \gls{MID} $G_{max} \in\mathcal{C}$ is said to be \textit{maximal} if, for any $i\in[n]$, $(Y_j,Y_i)\in E(G_{max})$ for all $j\in [i-1]$.
The number of edges in $E(G_{max} )$ terminating into random vertices  is denoted with $n_{max}$. 
\end{enumerate}
\end{definition}


The minimal \gls{MID} can be thought of  as the \gls{MID} including the highest number of conditional independence statements respecting the partial order over $\{Y_1,\dots,Y_n\}$. The maximal \gls{MID} on the other hand can be seen as a saturated model, implying no conditional independence statements. 
The minimal and maximal \glspl{MID} are particularly important since all the other elements of $\mathcal{C}$ can be seen as extensions or simplifications of the minimal and maximal \glspl{MID} respectively. 


\begin{example}
Figures~\ref{fig-min} and~\ref{fig-max} show the minimal and the maximal \gls{MID} respectively of the equivalence class including the \gls{MID} of Figure~\ref{fig-ex}.
\end{example}

%Let $G_{min}$ be the minimal \gls{MID} of an equivalence class $\mathcal{C}$ and let $n_{min}$ be the number of edges $(Y_i,Y_j)$ in $E(G_{min})$, for $i\in [n]$ and $j\in \mathbb{V}$.

\begin{proposition}
\label{lattice}
The \glspl{MID} in $\mathcal{C}$ form a bounded distributive ranked lattice whose meet and  join correspond respectively to the intersection and the union of the edge sets. The greatest element of the lattice is the maximal \gls{MID}, whilst its least element is the minimal \gls{MID}. The rank is determined by the difference between the number of edges $(Y_j,Y_i)$, $j<i$, $i\in\mathbb{V}$, and $n_{min}$.
\end{proposition}
\begin{proof}
This result follows directly by observing that  \textcolor{blue}{equivalent \glspl{MID} can be thought of as a power set over the edge set of the diagrams. DOVREMMO METTERLO PIU' IN EVIDENZA PERCHE' E' L'ELEMENTO FONDAMENTALE SU CUI SI REGGE QUESTA SECTION. PER ESEMPIO, 1. POSSIBILMENTE CON QUALCHE 'SCHEMA' (AAA) and 2. POSSIAMO FARE RIFERIMENTO AI MODELLI GRAFICI?  SUBITO DOPO LA PROPOSIZIONE DOVREMMO METTERE UN SEMPLICE ESEMPIO CON POCHISSIMI NODI, ESPLICITARE LA RELAZIONE D'ORDINE E IL DIAGRAMMA DI HESSE} . 
\end{proof}


%This lattice, equivalently $\mathcal C$, can be rather large. 
The rank of $G_{min}$ and of $G_{max}$ are $n_{min}$ and $n_{max}=\sum_{i\in\mathbb{V}}i-1$, respectively
%Let  $G_{max}$ be the  maximal \gls{MID} of an equivalence class $\mathcal{C}$ and let  $n_{max}=\sum_{i\in\mathbb{V}}i-1 $ count the edges terminating into random vertices in the maximal \gls{MID}. 
and $\mathcal{C}$ has $n_{\mathcal{C}}=2^{n_{max}-n_{min}}$  elements. Although $n_\mathcal{C}$ is usually very large, its elements can be ordered accordingly to a highly regular lattice, as noted in Proposition~\ref{lattice}, thus providing some computational advantages that we outline below.

\begin{example}
Consider for example the equivalence class $\mathcal{C}$ comprising the \gls{MID} in Figure~\ref{fig-ex}. It is easy to deduce that for $\mathcal{C}$, $n_{min}=4$, $n_{max}=12$. This equivalence class then consists of 256 \glspl{MID}. We can further note that within this class there are $\binom{8}{i}$ \glspl{MID} having $n_{min}+i$ edges terminating into random vertices.
\end{example}

Analogously to lattices, we can now define the concept of a cover of one \gls{MID} with respect to another.
\begin{definition}
\label{cover}
Let $G$ and $G'$ be two \glspl{MID} in  $\mathcal{C}$. We say that $G'$ covers $G$ if for $i\in\mathbb{V}$, $j<i$, the edge set 
$E(G)$ is equal to $E(G')\cup \{(Y_j,Y_i)\}$ . 
\end{definition}


Theorem~\ref{basta} shows how the expected utility vectors of two \glspl{MID} in the same equivalence class differ if one \gls{MID} covers the other. 

\begin{theorem} 
Let $G$ and $G'$ be two \glspl{MID} in $\mathcal{C}$ and let $\bar{\bm{U}}_k$ and $\bar{\bm{U}}_k'$, $k\in[n]$, be conditional utility vectors for $G$ and $G'$ respectively. Assume $G'$ covers $G$, that for some $i,j\in [n]$ $E(G')= E(G)\cup \{(Y_j,Y_i)\}$ and that $\mathcal{S}(G^M)\neq \mathcal{S}(G'^M)$, i.e. the moralized versions of the \glspl{MID} have different skeletons. The dimension of the conditional expected utility vectors  $\bar{\bm{U}}_k'$ is equal to $c_kr_j$ for $k=i$ and for any $k=l$, where $l$ is the index of those vertices such that $(Y_j,Y_l)\not \in E(G)$, $j<l<i$. 
\label{basta}
\end{theorem}
\begin{proof}Again it follows directly by identifying the sets $A(i)$ for which the two \glspl{MID} differ.  \end{proof}
Theorem~\ref{basta} implies that if two \glspl{MID} have the same skeleton, then not only the entries of their conditional expected utility vectors have the same polynomial structure, but the vectors have also the exact same dimension. 

\textcolor{blue}{
The lattice over the elements of $\mathcal{C}$ respects the Jordan-Dedeking chain condition ??da dimostrare?? and Corollary~\ref{latcor} shows}
 how the expected utility vectors of  $G$ and $G^{'}$ differ if $G\prec G^{'}$, where $\prec$ denotes the order relationships associated to \textcolor{blue}{$\mathcal C$ ??QUESTO SIMBOLO DOVREBBE ESSERE SCRITTO PRIMA}.
 
\begin{corollary}
\label{latcor}
Let $G$ and $G'$ be two \glspl{MID} be in $\mathcal{C}$ and let $G\prec G^{'}$. Then the difference between the expected utility vectors associated to $G$ and $G^{'}$ computed by Algorithm~\ref{algo} is equal to the union of the differences between any pair $G_i$, $G_j$ of elements of a maximal chain between $G$ and $G^{'}$.
\end{corollary}
Corollary~\ref{latcor} can also be used to deduce how the conditional expected utilities of any two \glspl{MID}, $G$ and $G'$ in $\mathcal{C}$ differ at any stage of their evaluations. First, the differences between $G$ and $G\land G'$ are deduced from Corollary~\ref{latcor}. Then the same procedure is iterated for  $G\land G'$ and $G'$. 
We do not fully explore here the computational advantages associated to  the lattice structure of $\mathcal C$, but we present an example of how this lattice can help a \gls{DM} when eliciting the edge set of an \gls{MID}. From a computational point of view, Theorem~\ref{basta} guarantees that the evaluation of two equivalent \glspl{MID} with the same skeleton requires the same number of operations. Therefore, if a \gls{DM} is not sure whether a certain conditional independence statement holds or not, the edge associated to such statement can be simply added to the diagram, given that the resulting \gls{MID} has the same skeleton.
\end{comment}
\subsubsection{Asymmetric decision problems.}
\label{asymmetry}
In Section \ref{sec:asy} we claimed that most of the graphical representations of asymmetric decision problems lose the intuitiveness associated to \glspl{ID}. In this section we start to demonstrate one of the great advantages of symbolic approaches, namely that the complexity of both the polynomial structure and symbolic definition of an asymmetric problem is simpler than the one of the symmetric embedding problem. We discuss in greater length this feature in Section \ref{sec:diff} below. 

Here we characterize an asymmetry between two chance nodes which, depending on the stage of the evaluation,  may  entail setting equal to zero monomials in either some or all  the rows of the conditional expected utility vector.  This therefore implies that the polynomial structure of the conditional expected utility vectors and at times also their dimension change. We present the result for elementary asymmetries of the following form: if $Y_i=y_i$ then $Y_j\neq y_j$. Composite asymmetries are unions of simple asymmetries and the features of the conditional expected utility vectors in more general cases can be deduced through a sequential application of Theorem~\ref{polyasy}.

\begin{theorem}
\label{polyasy}
Let $\Gr$ be an \gls{MID}, $Y_i$ and $Y_j$ be two random variables with $j>i$, $U_x$ be the first utility node that follows $Y_j$ in the \gls{DS}. Assume the asymmetry $Y_i=y_i\Rightarrow Y_j\neq y_j$ holds and that $k$ and $z$ are the highest indices such that $j\in B_k$ and $i\in B_z$ and assume  $k>j$. Then 
\begin{itemize}
\item for $t\in[z]_j$, each row of $\bar{\bm{u}}_t$ has $\prod_{s\in B_t\setminus \{i\cup j\}} r_s$ rows with no monomials;
\item for $t\in[j]_i$, each row of $\bar{\bm{u}}_t$ has $\prod_{s\in B_t\setminus \{i\}} r_s$ rows with polynomials of a different structure. Specifically, these consists of, in the notation of Theorem~\ref{polyexp}, $r_{tba}'$ monomials of degree $d_{tba}$, where for $a\in[m]_{x-1}$ and $b\in[a]_{l-1}$ 
\[
r_{tba}'=\left(\binom{a-x}{b-l}-1\right)\prod_{s\in[j_a]_{t-1}}r_s/r_j;
\]  
\item for $t\in[i]$, each row of $\bar{\bm{u}}_t$ has in the notation of Theorem~\ref{polyexp}, $r_{tba}''$ monomials of degree $d_{tba}$, where for $a\in[m]_{x-1}$ and $b\in[a]_{l-1}$ 
\[
r_{tba}''=\left(\binom{a-x}{b-l}-1\right)\prod_{s\in[j_a]_{t-1}}r_s/(r_j\cdot r_i).
\]  
\end{itemize}

\end{theorem}
The proof of this theorem is provided in Appendix \ref{cippu}.
{Corollary~\ref{corro} gives a  characterization of simple asymmetries between any two variables, whether they are controlled or non-controlled. This follows  from Theorem~\ref{polyasy} since controlled variables can be thought of as a special case of random ones.

\begin{corollary}\label{corro}
In the notation of Theorem~\ref{polyexp} and under the assumptions of Theorem~\ref{polyasy}, with the difference that $Y_i$ and $Y_j$ are two variables, controlled or non-controlled, we have that
\begin{itemize}
\item for $t\in[z]_j$, each row of $\bar{\bm{u}}_t$ has $\prod_{s\in B_t\setminus \{i\cup j\}} r_s$ rows with no monomials;
\item for $t\in[j]_i$, $\bar{\bm{u}}_t$ has at most $\prod_{s\in B_t\setminus \{i\}} r_s$ rows with polynomials of a different structure. Specifically, these consists of between $r_{tba}'$ and $r_{tba}$ monomials of degree $d_{tba}$, for $a\in[m]_{x-1}$ and $b\in[a]_{l-1}$;
\item  for $t\in[i]$, some rows of $\bar{\bm{u}}_t$ have a number of monomials of degree $d_{tba}$ between $r_{tba}''$ and $r_{tba}$, for $a\in[m]_{x-1}$ and $b\in[a]_{l-1}$.
\end{itemize} 
\label{corasy}
\end{corollary}


\begin{example}[Example~\ref{eee} continued]
 Consider the asymmetric problem described in Figure \ref{sid}. The symbolic imposition of the asymmetric constraints reduces from ten to three the number of monomials in $\bar{u}_5$ which becomes 
\[
k_3\theta_{311}p_{6111}p_{511i}+k_2\theta_{21}p_{511i}+hk_2k_3\theta_{311}\theta_{21}p_{6111}p_{511i}, \;\;\;\;\; i\in[1]^0.
\]
Suppose the EUMaximization suggested that $Y_4=0$ is optimal if $Y_3=1$ and that $Y_4=1$ is preferred if $Y_3=0$. The entry of $\bar{u}_3$ for which $Y_2=1$ and $Y_1=1$ can be written as
\begin{multline*}
\sum_{i,\in[1]^0}((k_2\theta_{21}+k_3\theta_{311}p_{6111}(1+hk_2\theta_{21}))p_{5110}p_{3011}+k_1\theta_{1i}p_{3i11}\\
+hk_1k_3\theta_{11}p_{5101}p_{3111}((1+k_2\theta_{21})\theta_{3j0}p_{6j10})).
\end{multline*}
This polynomial has only nine monomials, which, compared to $42$ - the number of monomials in the symmetric case - means a reduction  of the number of monomials to over one quarter.
\end{example}

The example above showed that under asymmetries the polynomial representation is simpler but still able to inform decision centres about the necessary parameters to elicit. This is one of the great advantages of our symbolic method.  Finally it is possible to develop a variant of Algorithm~\ref{algo1} which explicitly takes into account the asymmetries of the problem during its evaluation.  This variant would  be computationally more efficient, since it would require the computation of a smaller number of monomials and, possibly, polynomials.
 
\subsection{A Differential Approach to Inference in Staged Trees}
\label{sec:diff}

\subsubsection{Symbolic Definition of Staged Trees.}
As discussed in Section \ref{sec:history}, symbolic methods have proved useful in \glspl{BN}, although these techniques do come with a considerable computational cost. In this section we develop a symbolic approach for \textit{staged trees} (Section \ref{sec:tree}) and we demonstrate that such difficulties are eased, because of the more intuitive parametrization associated to these models. In addition because of the wide variety of possible hypotheses they embody, staged trees are necessarily models over much smaller state spaces than \glspl{BN}. Since this is the main computational issue for symbolic approaches associated with \glspl{BN}, it follows that trees can be very practical contexts to investigate symbolic inferential queries.

Following \citet{Darwiche2003}, \citet{Gorgen2015} and \citet{Pistone2001} we define the \textbf{interpolating polynomial} of a staged tree.

\begin{definition}\label{def:interpol}
Let $\mathcal{T}=(V,E)$ be a staged tree with primitive probabilities $\theta_e$, $e\in E$, and set of root-to-leaf paths $\Lambda(s_0,\mathcal{T})$. We call $\Lambda(e)=\{r\in\Lambda(s_0,\mathcal{T})\;|\; e\in E(r)\}$ an \emph{edge centred} event, and set 
$\lambda_e(r)$, for $e\in E$, to be an indicator of $r\in\Lambda(e)$. We call
\begin{equation*}\label{eq:interpol}
c_{\mathcal{T}}(\theta,\lambda)=\sum_{\mathclap{r\in R_\T~}}~\prod_{\mathclap{\quad e\in E(r)}}\lambda_e(r)\theta_e
\end{equation*}
the \textbf{interpolating polynomial} of $\mathcal{T}$.
\end{definition}

The interpolating polynomial is a sum of probabilities of atomic events with indicators for certain conditional events happening or not happening. Even though all these unknowns sum to one, in our symbolic approach we treat them just like indeterminates. Note that the indicators $\lambda_e(r)$ on the edges $e\in E(r)$ are associated to the (conditional) event represented by $e$, having probability $\theta_e$. This notation is apparently redundant, but will turn out to be useful below. We observe that this redundancy is one of the great advantages of a staged tree: whilst \cite{Darwiche2003} needs to compute conditional probabilities of all \emph{compatible parent structures} of an event, which is a rather obscure concept in a symbolic framework, and \cite{Castillo1996a} computes the product space of any indeterminates' combination regardless of their meaning, a tree visualisation of our model gives us the necessary structure immediately: events can be simply read from the paths in the graph. 

\begin{example}
The interpolating polynomial of the tree in Figure \ref{fig:ET}, not considering the underlying stage structure, is equal to (for ease of notation we omit the indicator functions) 
\begin{equation}\label{eq:interpolstaged}
c_\mathcal{T}'(\theta)=\theta_{01}\theta_{11}\theta_{31}
+\theta_{01}\theta_{11}\theta_{30}
+\theta_{01}\theta_{10}\theta_{41}
+\theta_{01}\theta_{10}\theta_{40}
+\theta_{00}\theta_{21}
+\theta_{00}\theta_{20}.
\end{equation}
When plugging in the conditional independence constraints, we obtain the interpolating polynomial of the staged tree as
\begin{align}\label{eq:interpolceg}
c_\T(\theta)&=\theta_{01}\theta_{11}\theta_{31}
+\theta_{01}\theta_{11}\theta_{30}
+\theta_{01}\theta_{10}\theta_{31}
+\theta_{01}\theta_{10}\theta_{30}
+\theta_{00}\theta_{11}
+\theta_{00}\theta_{10},\\
&~=~\theta_{01}(\theta_{11}(\theta_{31}+\theta_{30})+\theta_{10}(\theta_{31}+\theta_{30}))+\theta_{00}(\theta_{11}+\theta_{10}).\label{eq:factored}
\end{align}
Conversely, the interpolating polynomial of the \gls{BN} representation of the problem described by this staged tree, ignoring the conditional independence structure, equals to
\begin{equation}\label{eq:interpolBN}
\begin{split}
c_{\text{BN}}(\theta)=&
\theta_{01}\theta_{11}\theta_{31}
+\theta_{01}\theta_{11}\theta_{30}
+\theta_{01}\theta_{10}\theta_{41}
+\theta_{01}\theta_{10}\theta_{40}\\
&+\theta_{00}\theta_{21}\theta_{51}
+\theta_{00}\theta_{21}\theta_{50}
+\theta_{00}\theta_{20}\theta_{61}
+\theta_{00}\theta_{20}\theta_{60},
\end{split}
\end{equation}
\end{example}

We now look at interpolating polynomials from an algebraic viewpoint. Because of the product space structure of the sample space of a \gls{BN}, the interpolating polynomial of a \gls{BN} with $n$ vertices equals the sum of  monomials each of which is of degree $2n$ and so \emph{homogeneous}. Moreover, the stage structure of a \gls{BN}-induced tree as in Figure \ref{fig:ETBN} is such that no two vertices along the same directed path are in the same stage, in fact stages exist only along \emph{orthogonal cuts} \citep{Thwaites2015}. Thus in particular, the interpolating polynomial of a \gls{BN} is also square free. This is shown for instance by the interpolating polynomial $c_{\text{BN}}$ in equation (\ref{eq:interpolBN}),  an homogeneous polynomial of degree $3$ whose number of terms equals the number of paths in its tree representation.  Importantly this polynomial has been simply read from the event tree by first multiplying over all primitive probabilities along one root-to-leaf path, and then summing over all of these paths. This is a lot easier done using Figure \ref{fig:ETBN} than in the associate \gls{BN} representation, where we would have had to sum over compatible parent configurations.

Conversely, interpolating polynomials of staged trees are not necessarily homogeneous and  square free. For instance, the interpolating polynomial in equation (\ref{eq:interpolceg}) is now inhomogeneous with total degree $3$ and monomial terms having degree $2$ or $3$. Although our staged tree examples have a square free structure, non multilinear interpolating polynomials arise from tree where two stages are in the same root-to-leaf path \citep[as shown in][]{Gorgen2015}. For the purposes of this section we focus only on staged trees whose interpolating polynomial is square free.

 Notice that $c_\mathcal{T}$ can be easily factorised in (\ref{eq:factored}) by simply following the structure of the underlying graph. In \cite{Brandherm2004} polynomials of this type are called \textit{factored}. This representation entails great computational advantages since the compilation into an \gls{AC} is almost instantaneous. Whilst for \glspl{BN} the factored representation might be difficult to obtain, it comes almost for free in tree models. 

We observe that the graphical simplicity of a staged tree model in comparison to an uncoloured tree or a \gls{BN} is also reflected algebraically: the polynomial in (\ref{eq:interpolceg}) has fewer indeterminates than the one in (\ref{eq:interpolstaged}) and a lot fewer than the polynomial associated to a tree which is derived from a \gls{BN} in (\ref{eq:interpolBN}). This is because in the \gls{BN} the redundancy of atoms gives rise to redundant terms. 

The definition of interpolating polynomials allows us to compute the probability of any event associated to a tree.
\begin{lemma}\label{lem:darwiche}
For any event $A$ represented by a set of root-to-leaf paths $R_A$ in a staged tree $\mathcal{T}$,
\begin{equation*}\label{eq:lem1}
\mathbb{P}(A)=\sum_{\mathclap{r\in R_A~}}~\prod_{\mathclap{\quad e\in E(r)}}\lambda_e(r)\theta_e=c_\T(\theta,\lambda|_{R_A}),
\end{equation*}
where $\lambda|_{R_A}$ indicates that $\lambda_e(r)=1$ for all $e\in E(r)$ with $r\in R_A$, and else zero.
\end{lemma} 

\begin{example}\label{bsp:lem1}
Suppose we are interested in calculating the probability of  political disruption. This is captured by the set $R_A=\Lambda(e_{31})\cup\Lambda(e_{41})$ corresponding to all root-to-leaf paths going through an edge labelled 'yes' which translates in  summing all terms in (\ref{eq:interpolceg}) including $\theta_{31}$. Therefore $\mathbb{P}(A)=\theta_{01}\theta_{11}\theta_{31}+\theta_{01}\theta_{12}\theta_{31}$, again omitting the $\lambda$ indicators.
\end{example}

\subsubsection{The Differential Approach.}\label{sect:diff}
We are now able to provide a probabilistic semantic, just as \cite{Darwiche2003} for \glspl{BN}, to the derivatives of  polynomials associated to staged trees. More formally, and as shown in \cite{Thwaites2015}, events of the type $\Lambda(e)$ can be formally represented by defining new auxiliary indicator variables and then conditioning variables on these. For ease of notation we let in this section $\lambda_e=\lambda_e(r)$.

\begin{proposition}\label{prop:darwiche}
For equally coloured edges $e\in E$ and an event $A$ represented by the root-to-leaf paths $R_A$, the following results hold:
\begin{equation}
\label{eq:der1}
\mathbb{P}(\Lambda(e)|A)=\frac{1}{c_\mathcal{T}(\theta,\lambda|_{R_A})}\frac{\partial c_\mathcal{T}(\theta,\lambda|_{R_A})}{\partial \lambda_e},~\;\;\;
\mathbb{P}(\Lambda(e),A)=\theta_e\frac{\partial c_\mathcal{T}(\theta,\lambda|_{R_A})}{\partial \theta_e}.
\end{equation}
\end{proposition}
All the probabilities in (\ref{eq:der1}) are equal to zero whenever $e\not\in E(r)$ for all $r\in R_A$. Notice that the derivatives of tree polynomials have the exact same interpretation of the ones of \glspl{BN} as in \cite{Darwiche2003}. Here we restricted our attention to square free staged trees but analogous results hold in the generic case: each monomial with indeterminate $\lambda_e$ and $\theta_e$ of degree higher than one would need to be differentiated a number of times equal to the degree of that indeterminate.

\begin{proposition} \label{prop:darwiche2}
In the notation of Proposition \ref{prop:darwiche}, the following holds for $e_1, e_2\in E$:
\begin{align}
\mathbb{P}(\Lambda(e_1),\Lambda(e_2)\;|\; A)&~=~ \frac{1}{c_\mathcal{T}(\theta,\lambda|_{R_A})}\frac{\partial^2 c_\mathcal{T}(\theta,\lambda|_{R_A})}{\partial \lambda_{e_1}\partial\lambda_{e_2}}, \\
\mathbb{P}(\Lambda(e_1),\Lambda(e_2),A)&~=~\theta_{e_1}\theta_{e_2}\frac{\partial c_\mathcal{T}(\theta,\lambda|_{R_A})}{\partial \theta_{e_1}\partial \theta_{e_2}},\\
\mathbb{P}(A\;|\; \Lambda(e))&~=~\frac{\partial^2 c_\mathcal{T}(\theta,\lambda|_{R_A})}{\partial \theta_e \partial \lambda_e}. 
\end{align}
\end{proposition}
It is an easy exercise to deduce from Proposition \ref{prop:darwiche2} the probabilistic meaning of higher order derivatives.

Importantly, in the staged tree model class derivatives can be associated to causal propositions as formalised in \cite{Thwaites2010}. Note that such a result does not hold in general for the polynomials describing \gls{BN} probabilities. 

\begin{proposition}
Suppose the staged tree is believed to be causal as in \cite{Thwaites2010}. Then under the notation of Proposition\ref{prop:darwiche2},
\begin{equation}
\label{eq:causal}
\mathbb{P}(A\;||\; \Lambda(e))=\frac{\partial^2 c_\mathcal{T}(\theta,\lambda|_{R_A})}{\partial \theta_e \partial \lambda_e}
\end{equation}
is the probability of the event $A$ when the system is forced to go through edge $e$.
\end{proposition}

Note that all the quantities in (\ref{eq:der1})-(\ref{eq:causal}) can be used in sensitivity analysis, for instance by investigating the changes in probability estimates when the system is set to be in a certain scenario of interest.

\begin{example}\label{bsp:derivatives}
We now compute a set of derivatives of the interpolating polynomial $c_\mathcal{T}$ in (\ref{eq:interpolceg}) with respect to $\lambda_{31}$ and $\theta_{31}$ to perform probabilistic inference over the event $A$ of political disruption. Thus, we consider the edge $e=(v_3,v_7)$ and
\begin{align}
\frac{1}{c_\mathcal{T}(\theta,\lambda|_{R_A})}\frac{\partial c_\mathcal{T}(\theta,\lambda|_{R_A})}{\partial \lambda_e}&~=~\frac{\theta_{01}\theta_{11}\theta_{31}+\theta_{01}\theta_{12}\theta_{31}}{\theta_{01}\theta_{11}\theta_{31}+\theta_{01}\theta_{12}\theta_{31}}=1,\label{eq:1deriv_lambda}\\
\theta_e\frac{\partial c_\T(\theta,\lambda|_{R_A})}{\partial \theta_e}&~=~\theta_{13}(\theta_{01}\theta_{11}+\theta_{01}\theta_{12})=\mathbb{P}(A)\label{eq:1deriv_theta},\\
\frac{\partial^2 c_\T(\theta,\lambda|_{R_A})}{\partial \theta_e \partial \lambda_e}&~=~\theta_{01}\theta_{11}+\theta_{01}\theta_{12}.\label{eq:2deriv_lambdatheta}
\end{align}
Observe that (\ref{eq:1deriv_lambda}) is equal to one since every path associated to the event $A$ must go through $e$. From the same argument follows that (\ref{eq:1deriv_theta}) is equal to $P(A)$. Eq. (\ref{eq:2deriv_lambdatheta}) is a simple consequence of Bayes theorem, which can be checked algebraically. 
\end{example}

\subsubsection{Trees as Circuits.}\label{sect:circuit}
The previous sections have introduced a comprehensive symbolic inferential toolbox for trees, based on the computation of the interpolating polynomial and its derivatives. In \cite{Darwiche2003} it is shown that an efficient method to compute such polynomials is by representing them as an \textbf{\gls{AC}}. This is a \gls{DAG} whose leaves are the indeterminates and the inner nodes are labelled by multiplication and summation operations. The \emph{size} of  the circuit is measured by its number of edges.

\glspl{AC} of staged tree polynomials are smaller in size than the ones associated to \glspl{BN} for two reasons: first, a tree might have fewer root-to-leaf paths (as in our example); second, there can be less indeterminates because unfoldings with probability zero are not included in the model and coloured labels further decrease the number of indeterminates. Therefore, in asymmetric settings we can expect computations to be much faster for trees than \glspl{BN}.

A major problem in the compilation of \glspl{BN} polynomials consists in the identification of the \gls{AC} of smallest size. This usually entails the computation of the \gls{BN}'s jointree and the application of rather complicated algorithms \citep{Darwiche2003}. We note here that in tree models this is straightforward since the interpolating polynomial is naturally factored.

\begin{example}
Notice that equation (\ref{eq:factored}) can be rewritten as
$
c_\T(\theta)=\theta_{01}(\theta_{11}+\theta_{12})(\theta_{31}+\theta_{31})+\theta_{02}(\theta_{11}+\theta_{12}).
$

This gives us the \gls{AC} in Figure \ref{fig:arthcirc} where leaves with the same parent are labelled by primitive probabilities from the same floret, and labels belonging to leaves in the tree are first summed in the \gls{AC}. It is easy to deduce that the \gls{AC} associated to the \gls{BN}'s polynomial in equation (\ref{eq:interpolBN}) would be much larger than the one in Figure \ref{fig:arthcirc}. We note also that, whilst all the \glspl{AC} deriving from \glspl{BN} in \cite{Darwiche2003} are trees, ours is more generally a \gls{DAG}.

\begin{figure}[tb]
\[\xymatrixcolsep{0.7pc}\xymatrixrowsep{0.2pc}{\xymatrix{
&&&& +\ar[ld]\ar[rrd]\\
&&& \star\ar[llld]\ar[ld] \ar[drr]&&& \star\ar[ld]\ar[rd] \\
\theta_{01}&& +\ar[ld]\ar[rd] &&& +\ar[ld]\ar[rd] &&\theta_{02}\\
&\theta_{31}&&\theta_{32}&\theta_{11}&&\theta_{12}
}}\]\vspace*{-10pt}%reducing extra space caused by \[...\]
\caption[]{The arithmetic circuit of the polynomial in equation (\ref{eq:interpolceg}).}\label{fig:arthcirc}
\end{figure}
\end{example}

\section{Conclusions}
In this chapter, after discussing the links between algebraic and symbolic inferential methods, we have developed algebraic formal methods to investigate adequacy in \glspl{IDSS}. We have been able to show that new separation conditions consisting of certain polynomial relations can guarantee adequacy and distributivity in general and in variety of examples. In particular the application of such methods to a particular class of \gls{BN} models lead us to deduce formulae for the computation of moments of not only additive, but also multilinear functions of random vectors over an agreed \gls{DAG}. We then showed how symbolic inferential and decision models can find application in certain structural consensus and deduced important features and properties of this approach. 
